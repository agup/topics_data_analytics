{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from scipy import misc\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- words.txt ---------------------------------------------------------------#\n",
      "\n",
      "#\n",
      "\n",
      "# iam database word information\n",
      "\n",
      "#\n",
      "\n",
      "# format: a01-000u-00-00 ok 154 1 408 768 27 51 AT A\n",
      "\n",
      "#\n",
      "\n",
      "#     a01-000u-00-00  -> word id for line 00 in form a01-000u\n",
      "\n",
      "#     ok              -> result of word segmentation\n",
      "\n",
      "#                            ok: word was correctly\n",
      "\n",
      "#                            er: segmentation of word can be bad\n",
      "\n",
      "#\n",
      "\n",
      "#     154             -> graylevel to binarize the line containing this word\n",
      "\n",
      "#     1               -> number of components for this word\n",
      "\n",
      "#     408 768 27 51   -> bounding box around this word in x,y,w,h format\n",
      "\n",
      "#     AT              -> the grammatical tag for this word, see the\n",
      "\n",
      "#                        file tagset.txt for an explanation\n",
      "\n",
      "#     A               -> the transcription for this word\n",
      "\n",
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meta_words = '/Users/arushigupta/Desktop/IAM/ascii/words.txt'\n",
    "word_l =[]\n",
    "loc_l = []\n",
    "info = open(meta_words, \"r\")\n",
    "for li in info:\n",
    "    if li.startswith('#'):\n",
    "        print(li)\n",
    "    else:\n",
    "        #print(li.split(' '))\n",
    "        li_sp = li.split(' ')\n",
    "        #print(li_sp[8].strip())\n",
    "        addr = li_sp[0].split('-')\n",
    "        to_word = '/Users/arushigupta/Desktop/IAM/words/'\n",
    "        to_word += addr[0] + '/'\n",
    "        to_word += addr[0] + '-' + addr[1] + '/'\n",
    "        to_word += addr[0] + '-' + addr[1] + '-' + addr[2] + '-' + addr[3] + '.png'\n",
    "        #print(to_word)\n",
    "        word_l.append(li_sp[8].strip())\n",
    "        loc_l.append(to_word.strip())\n",
    "info.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'MOVE',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'Mr.',\n",
       " 'Gaitskell',\n",
       " 'from',\n",
       " 'nominating',\n",
       " 'any',\n",
       " 'more',\n",
       " 'Labour',\n",
       " 'life',\n",
       " 'Peers',\n",
       " 'is',\n",
       " 'to',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'a',\n",
       " 'meeting',\n",
       " 'of',\n",
       " 'Labour',\n",
       " 'M',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Michael',\n",
       " 'Foot',\n",
       " 'has',\n",
       " 'put',\n",
       " 'down',\n",
       " 'a',\n",
       " 'resolution',\n",
       " 'on',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'and',\n",
       " 'he',\n",
       " 'is',\n",
       " 'to',\n",
       " 'be',\n",
       " 'backed',\n",
       " 'by',\n",
       " 'Mr.',\n",
       " 'Will',\n",
       " 'Griffiths',\n",
       " ',',\n",
       " 'M',\n",
       " 'for',\n",
       " 'Manchester',\n",
       " 'Exchange',\n",
       " '.',\n",
       " 'A',\n",
       " 'MOVE',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'Mr.',\n",
       " 'Gaitskell',\n",
       " 'from',\n",
       " 'nominating',\n",
       " 'any',\n",
       " 'more',\n",
       " 'Labour',\n",
       " 'life',\n",
       " 'Peers',\n",
       " 'is',\n",
       " 'to',\n",
       " 'be',\n",
       " 'made',\n",
       " 'at',\n",
       " 'a',\n",
       " 'meeting',\n",
       " 'of',\n",
       " 'Labour',\n",
       " '0M',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Michael',\n",
       " 'Foot',\n",
       " 'has',\n",
       " 'put',\n",
       " 'down',\n",
       " 'a',\n",
       " 'resolution',\n",
       " 'on',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'and',\n",
       " 'he',\n",
       " 'is',\n",
       " 'to',\n",
       " 'be',\n",
       " 'backed',\n",
       " 'by',\n",
       " 'Mr.',\n",
       " 'Will',\n",
       " 'Griffiths',\n",
       " ',',\n",
       " '0M',\n",
       " 'for',\n",
       " 'Manchester',\n",
       " 'Exchange',\n",
       " '.',\n",
       " 'Though',\n",
       " 'they',\n",
       " 'may',\n",
       " 'gather',\n",
       " 'some',\n",
       " 'Left-wing',\n",
       " 'support',\n",
       " ',',\n",
       " 'a',\n",
       " 'large',\n",
       " 'majority',\n",
       " 'of',\n",
       " 'Labour',\n",
       " 'M',\n",
       " 'are',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'Foot-Griffiths',\n",
       " 'resolution',\n",
       " '.',\n",
       " 'Mr.',\n",
       " \"Foot's\",\n",
       " 'line',\n",
       " 'will',\n",
       " 'be',\n",
       " 'that',\n",
       " 'as',\n",
       " 'Labour',\n",
       " 'M',\n",
       " 'opposed',\n",
       " 'the',\n",
       " 'Government',\n",
       " 'Bill',\n",
       " 'which',\n",
       " 'brought',\n",
       " 'life',\n",
       " 'peers',\n",
       " 'into',\n",
       " 'existence',\n",
       " ',',\n",
       " 'they',\n",
       " 'should',\n",
       " 'not',\n",
       " 'now',\n",
       " 'put',\n",
       " 'forward',\n",
       " 'nominees',\n",
       " '.',\n",
       " 'He',\n",
       " 'believes',\n",
       " 'that',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Lords',\n",
       " 'should',\n",
       " 'be',\n",
       " 'abolished',\n",
       " 'and',\n",
       " 'that',\n",
       " 'Labour',\n",
       " 'should',\n",
       " 'not',\n",
       " 'take',\n",
       " 'any',\n",
       " 'steps',\n",
       " 'which',\n",
       " 'would',\n",
       " 'appear',\n",
       " 'to',\n",
       " '\"',\n",
       " 'prop',\n",
       " 'up',\n",
       " '\"',\n",
       " 'an',\n",
       " 'out-dated',\n",
       " 'institution',\n",
       " '.',\n",
       " 'Though',\n",
       " 'they',\n",
       " 'may',\n",
       " 'gather',\n",
       " 'some',\n",
       " 'Left-wing',\n",
       " 'support',\n",
       " ',',\n",
       " 'a',\n",
       " 'large',\n",
       " 'majority',\n",
       " 'of',\n",
       " 'Labour',\n",
       " 'M',\n",
       " 'are',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'Foot-',\n",
       " 'Griffiths',\n",
       " 'resolution',\n",
       " '.',\n",
       " 'Mr.',\n",
       " \"Foot's\",\n",
       " 'line',\n",
       " 'will',\n",
       " 'be',\n",
       " 'that',\n",
       " 'as',\n",
       " 'Labour',\n",
       " 'M',\n",
       " 'opposed',\n",
       " 'the',\n",
       " 'Government',\n",
       " 'Bill',\n",
       " 'which',\n",
       " 'brought',\n",
       " 'life',\n",
       " 'peers',\n",
       " 'into',\n",
       " 'existence',\n",
       " ',',\n",
       " 'they',\n",
       " 'should',\n",
       " 'not',\n",
       " 'now',\n",
       " 'put',\n",
       " 'forward',\n",
       " 'nominees',\n",
       " '.',\n",
       " 'He',\n",
       " 'believes',\n",
       " 'that',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Lords',\n",
       " 'should',\n",
       " 'be',\n",
       " 'abolished',\n",
       " 'and',\n",
       " 'that',\n",
       " 'Labour',\n",
       " 'should',\n",
       " 'not',\n",
       " 'take',\n",
       " 'any',\n",
       " 'steps',\n",
       " 'which',\n",
       " 'would',\n",
       " 'appear',\n",
       " 'to',\n",
       " '\"',\n",
       " 'prop',\n",
       " 'up',\n",
       " '\"',\n",
       " 'an',\n",
       " 'out-',\n",
       " 'Though',\n",
       " 'they',\n",
       " 'may',\n",
       " 'gather',\n",
       " 'some',\n",
       " 'Left-wing',\n",
       " 'support',\n",
       " ',',\n",
       " 'a',\n",
       " 'large',\n",
       " 'majority',\n",
       " 'of',\n",
       " 'Labour',\n",
       " '0M',\n",
       " 'are',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'turn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'Foot-Griffiths',\n",
       " 'resolution',\n",
       " '.',\n",
       " 'Mr.',\n",
       " \"Foot's\",\n",
       " 'line',\n",
       " 'will',\n",
       " 'be',\n",
       " 'that',\n",
       " 'as',\n",
       " 'Labour',\n",
       " '0M',\n",
       " 'opposed',\n",
       " 'the',\n",
       " 'Govern-',\n",
       " 'ment',\n",
       " 'Bill',\n",
       " 'which',\n",
       " 'brought',\n",
       " 'life',\n",
       " 'peers',\n",
       " 'into',\n",
       " 'existence',\n",
       " ',',\n",
       " 'they',\n",
       " 'should',\n",
       " 'not',\n",
       " 'now',\n",
       " 'put',\n",
       " 'forward',\n",
       " 'nominees',\n",
       " '.',\n",
       " 'He',\n",
       " 'believes',\n",
       " 'that',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Lords',\n",
       " 'should',\n",
       " 'be',\n",
       " 'abolished',\n",
       " 'and',\n",
       " 'that',\n",
       " 'Labour',\n",
       " 'should',\n",
       " 'not',\n",
       " 'take',\n",
       " 'any',\n",
       " 'steps',\n",
       " 'which',\n",
       " 'would',\n",
       " 'appear',\n",
       " 'to',\n",
       " '\"',\n",
       " 'prop',\n",
       " 'up',\n",
       " '\"',\n",
       " 'an',\n",
       " 'out-dated',\n",
       " 'institution',\n",
       " '.',\n",
       " 'Since',\n",
       " '1958',\n",
       " ',',\n",
       " '13',\n",
       " 'Labour',\n",
       " 'life',\n",
       " 'Peers',\n",
       " 'and',\n",
       " 'Peeresses',\n",
       " 'have',\n",
       " 'been',\n",
       " 'created',\n",
       " '.',\n",
       " 'Most',\n",
       " 'Labour',\n",
       " 'sentiment',\n",
       " 'would',\n",
       " '#',\n",
       " 'still',\n",
       " 'favour',\n",
       " 'the',\n",
       " 'abolition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Lords',\n",
       " ',',\n",
       " 'but',\n",
       " 'while',\n",
       " 'it',\n",
       " 'remains',\n",
       " 'Labour',\n",
       " 'has',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'adequate',\n",
       " 'number',\n",
       " 'of',\n",
       " 'members',\n",
       " '.',\n",
       " 'THE',\n",
       " 'two',\n",
       " 'rival',\n",
       " 'African',\n",
       " 'Nationalist',\n",
       " 'Parties',\n",
       " 'of',\n",
       " 'Northern',\n",
       " 'Rhodesia',\n",
       " 'have',\n",
       " 'agreed',\n",
       " 'to',\n",
       " 'get',\n",
       " 'together',\n",
       " 'to',\n",
       " 'face',\n",
       " 'the',\n",
       " 'challenge',\n",
       " 'from',\n",
       " 'Sir',\n",
       " 'Roy',\n",
       " 'Welensky',\n",
       " ',',\n",
       " 'the',\n",
       " 'Federal',\n",
       " 'Premier',\n",
       " '.',\n",
       " 'Since',\n",
       " '1958',\n",
       " ',',\n",
       " '13',\n",
       " 'Labour',\n",
       " 'life',\n",
       " 'Peers',\n",
       " 'and',\n",
       " 'Peeresses',\n",
       " 'have',\n",
       " 'been',\n",
       " 'created',\n",
       " '.',\n",
       " 'Most',\n",
       " 'Labour',\n",
       " 'sentiment',\n",
       " 'would',\n",
       " 'still',\n",
       " 'favour',\n",
       " 'the',\n",
       " 'abolition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Lords',\n",
       " ',',\n",
       " 'but',\n",
       " 'while',\n",
       " 'it',\n",
       " 'remains',\n",
       " 'Labour',\n",
       " 'has',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'adequate',\n",
       " 'number',\n",
       " 'of',\n",
       " 'members',\n",
       " '.',\n",
       " 'THE',\n",
       " 'two',\n",
       " 'rival',\n",
       " 'African',\n",
       " 'Nationalist',\n",
       " 'Parties',\n",
       " 'of',\n",
       " 'Northern',\n",
       " 'Rhodesia',\n",
       " 'have',\n",
       " 'agreed',\n",
       " 'to',\n",
       " 'get',\n",
       " 'together',\n",
       " 'to',\n",
       " 'face',\n",
       " 'the',\n",
       " 'challenge',\n",
       " 'from',\n",
       " 'Sir',\n",
       " 'Roy',\n",
       " 'Welensky',\n",
       " ',',\n",
       " 'the',\n",
       " 'Federal',\n",
       " 'Premier',\n",
       " '.',\n",
       " 'Since',\n",
       " '1958',\n",
       " ',',\n",
       " '13',\n",
       " 'Labour',\n",
       " 'life',\n",
       " 'Peers',\n",
       " 'and',\n",
       " '#',\n",
       " 'Peeresses',\n",
       " 'have',\n",
       " 'been',\n",
       " 'created',\n",
       " '.',\n",
       " 'Most',\n",
       " 'Labour',\n",
       " 'sentiment',\n",
       " 'would',\n",
       " 'still',\n",
       " 'favour',\n",
       " 'the',\n",
       " 'abolition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'House',\n",
       " 'of',\n",
       " 'Lords',\n",
       " ',',\n",
       " 'but',\n",
       " 'while',\n",
       " 'it',\n",
       " 'remains',\n",
       " 'Labour',\n",
       " 'has',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'adequate',\n",
       " 'number',\n",
       " 'of',\n",
       " 'members',\n",
       " '.',\n",
       " 'THE',\n",
       " 'two',\n",
       " 'rival',\n",
       " 'African',\n",
       " 'Nationalist',\n",
       " 'Parties',\n",
       " 'of',\n",
       " 'Northern',\n",
       " 'Rhodesia',\n",
       " 'have',\n",
       " 'agreed',\n",
       " 'to',\n",
       " 'get',\n",
       " 'together',\n",
       " 'to',\n",
       " 'face',\n",
       " 'the',\n",
       " 'challenge',\n",
       " 'from',\n",
       " 'Sir',\n",
       " 'Roy',\n",
       " 'Welensky',\n",
       " ',',\n",
       " 'the',\n",
       " 'Federal',\n",
       " 'Premier',\n",
       " '.',\n",
       " 'Delegates',\n",
       " 'from',\n",
       " 'Mr.',\n",
       " 'Kenneth',\n",
       " \"Kaunda's\",\n",
       " 'United',\n",
       " 'National',\n",
       " 'Independence',\n",
       " 'Party',\n",
       " '(',\n",
       " '280,000',\n",
       " 'members',\n",
       " ')',\n",
       " 'and',\n",
       " 'Mr.',\n",
       " 'Harry',\n",
       " \"Nkumbula's\",\n",
       " 'African',\n",
       " 'National',\n",
       " 'Congress',\n",
       " '(',\n",
       " '400,000',\n",
       " ')',\n",
       " 'will',\n",
       " 'meet',\n",
       " 'in',\n",
       " 'London',\n",
       " 'today',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'a',\n",
       " 'common',\n",
       " 'course',\n",
       " 'of',\n",
       " 'action',\n",
       " '.',\n",
       " 'Sir',\n",
       " 'Roy',\n",
       " 'is',\n",
       " 'violently',\n",
       " 'opposed',\n",
       " 'to',\n",
       " 'Africans',\n",
       " 'getting',\n",
       " 'an',\n",
       " 'elected',\n",
       " 'majority',\n",
       " 'in',\n",
       " 'Northern',\n",
       " 'Rhodesia',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'Colonial',\n",
       " 'Secretary',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Iain',\n",
       " 'Macleod',\n",
       " ',',\n",
       " 'is',\n",
       " 'insisting',\n",
       " 'on',\n",
       " 'a',\n",
       " 'policy',\n",
       " 'of',\n",
       " 'change',\n",
       " '.',\n",
       " 'Delegates',\n",
       " 'from',\n",
       " 'Mr.',\n",
       " 'Kenneth',\n",
       " \"Kaunda's\",\n",
       " 'United',\n",
       " 'National',\n",
       " 'Independence',\n",
       " 'Party',\n",
       " '(',\n",
       " '280,000',\n",
       " 'members',\n",
       " ')',\n",
       " 'and',\n",
       " 'Mr.',\n",
       " 'Harry',\n",
       " \"Nkumbula's\",\n",
       " 'African',\n",
       " 'National',\n",
       " 'Congress',\n",
       " '(',\n",
       " '400,000',\n",
       " ')',\n",
       " 'will',\n",
       " 'meet',\n",
       " 'in',\n",
       " 'London',\n",
       " 'today',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'a',\n",
       " 'common',\n",
       " 'course',\n",
       " 'of',\n",
       " 'action',\n",
       " '.',\n",
       " 'Sir',\n",
       " 'Roy',\n",
       " 'is',\n",
       " 'violently',\n",
       " 'opposed',\n",
       " 'to',\n",
       " 'Africans',\n",
       " 'getting',\n",
       " 'an',\n",
       " 'elected',\n",
       " 'majority',\n",
       " 'in',\n",
       " 'Northern',\n",
       " 'Rhodesia',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'Colonial',\n",
       " 'Secretary',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Iain',\n",
       " 'Macleod',\n",
       " ',',\n",
       " 'is',\n",
       " 'insisting',\n",
       " 'on',\n",
       " 'a',\n",
       " 'policy',\n",
       " 'of',\n",
       " 'change',\n",
       " '.',\n",
       " 'Delegates',\n",
       " 'from',\n",
       " 'Mr.',\n",
       " 'Kenneth',\n",
       " \"Kaunda's\",\n",
       " 'United',\n",
       " 'National',\n",
       " 'Independence',\n",
       " 'Party',\n",
       " '(',\n",
       " '280,000',\n",
       " 'members',\n",
       " ')',\n",
       " 'and',\n",
       " 'Mr.',\n",
       " 'Harry',\n",
       " \"Nkumbula's\",\n",
       " 'African',\n",
       " 'National',\n",
       " 'Congress',\n",
       " '(',\n",
       " '400,000',\n",
       " ')',\n",
       " 'will',\n",
       " 'meet',\n",
       " 'in',\n",
       " 'London',\n",
       " 'today',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'a',\n",
       " 'common',\n",
       " 'course',\n",
       " 'of',\n",
       " 'action',\n",
       " '.',\n",
       " 'Sir',\n",
       " 'Roy',\n",
       " 'is',\n",
       " 'violently',\n",
       " 'opposed',\n",
       " 'to',\n",
       " 'Africans',\n",
       " 'getting',\n",
       " 'an',\n",
       " 'elected',\n",
       " 'majority',\n",
       " 'in',\n",
       " 'Northern',\n",
       " 'Rhodesia',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'Colonial',\n",
       " 'Secretary',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Iain',\n",
       " 'Macleod',\n",
       " ',',\n",
       " 'is',\n",
       " 'insisting',\n",
       " 'on',\n",
       " 'a',\n",
       " 'policy',\n",
       " 'of',\n",
       " 'change',\n",
       " '.',\n",
       " 'Sir',\n",
       " \"Roy's\",\n",
       " 'United',\n",
       " 'Federal',\n",
       " 'Party',\n",
       " 'is',\n",
       " 'boycotting',\n",
       " 'the',\n",
       " 'London',\n",
       " 'talks',\n",
       " 'on',\n",
       " 'the',\n",
       " \"Protectorate's\",\n",
       " 'future',\n",
       " '.',\n",
       " 'Said',\n",
       " 'Mr.',\n",
       " 'Nkumbula',\n",
       " 'last',\n",
       " 'night',\n",
       " ':',\n",
       " '\"',\n",
       " 'We',\n",
       " 'want',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'what',\n",
       " 'to',\n",
       " 'do',\n",
       " 'if',\n",
       " 'the',\n",
       " 'British',\n",
       " 'Government',\n",
       " 'gives',\n",
       " 'in',\n",
       " 'to',\n",
       " 'Sir',\n",
       " 'Roy',\n",
       " 'and',\n",
       " 'the',\n",
       " 'talks',\n",
       " 'fall',\n",
       " 'through',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'bound',\n",
       " 'to',\n",
       " 'be',\n",
       " 'demonstrations',\n",
       " '.',\n",
       " '\"',\n",
       " 'Yesterday',\n",
       " 'Sir',\n",
       " \"Roy's\",\n",
       " 'chief',\n",
       " 'aide',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Julius',\n",
       " 'Greenfield',\n",
       " ',',\n",
       " 'telephoned',\n",
       " 'his',\n",
       " 'chief',\n",
       " 'a',\n",
       " 'report',\n",
       " 'on',\n",
       " 'his',\n",
       " 'talks',\n",
       " 'with',\n",
       " 'Mr.',\n",
       " 'Macmillan',\n",
       " 'at',\n",
       " 'Chequers',\n",
       " '.',\n",
       " 'Sir',\n",
       " \"Roy's\",\n",
       " 'United',\n",
       " 'Federal',\n",
       " 'Party',\n",
       " 'is',\n",
       " 'boycotting',\n",
       " 'the',\n",
       " 'London',\n",
       " 'talks',\n",
       " 'on',\n",
       " 'the',\n",
       " \"Protectorate's\",\n",
       " 'future',\n",
       " '.',\n",
       " 'Said',\n",
       " 'Mr.',\n",
       " 'Nkumbula',\n",
       " 'last',\n",
       " 'night',\n",
       " ':',\n",
       " '\"',\n",
       " 'We',\n",
       " 'want',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'what',\n",
       " 'to',\n",
       " 'do',\n",
       " 'if',\n",
       " 'the',\n",
       " 'British',\n",
       " 'demonstrations',\n",
       " '.',\n",
       " '\"',\n",
       " 'Yesterday',\n",
       " 'Sir',\n",
       " \"Roy's\",\n",
       " 'chief',\n",
       " 'aide',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Julius',\n",
       " 'Greenfield',\n",
       " ',',\n",
       " 'telephoned',\n",
       " 'his',\n",
       " 'chief',\n",
       " 'a',\n",
       " 'report',\n",
       " 'on',\n",
       " 'his',\n",
       " 'talks',\n",
       " 'with',\n",
       " 'Mr.',\n",
       " 'Macmillan',\n",
       " 'at',\n",
       " 'Chequers',\n",
       " '.',\n",
       " 'Sir',\n",
       " \"Roy's\",\n",
       " 'United',\n",
       " 'Federal',\n",
       " 'Party',\n",
       " 'is',\n",
       " 'boycotting',\n",
       " 'the',\n",
       " 'London',\n",
       " 'talks',\n",
       " 'on',\n",
       " 'the',\n",
       " \"Protectorate's\",\n",
       " 'future',\n",
       " '.',\n",
       " 'Said',\n",
       " 'Mr.',\n",
       " 'Nkumbula',\n",
       " 'last',\n",
       " 'night',\n",
       " ':',\n",
       " '\"',\n",
       " 'We',\n",
       " 'want',\n",
       " 'to',\n",
       " 'discuss',\n",
       " 'what',\n",
       " 'to',\n",
       " 'do',\n",
       " 'if',\n",
       " 'the',\n",
       " 'British',\n",
       " 'Government',\n",
       " 'gives',\n",
       " 'in',\n",
       " 'to',\n",
       " 'Sir',\n",
       " 'Roy',\n",
       " 'and',\n",
       " 'the',\n",
       " 'talks',\n",
       " 'fall',\n",
       " 'through',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'bound',\n",
       " 'to',\n",
       " 'be',\n",
       " 'demonstrations',\n",
       " '.',\n",
       " '\"',\n",
       " 'Yesterday',\n",
       " 'Sir',\n",
       " \"Roy's\",\n",
       " 'chief',\n",
       " 'aide',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'Julius',\n",
       " 'Greenfield',\n",
       " ',',\n",
       " 'telephoned',\n",
       " 'his',\n",
       " 'chief',\n",
       " 'a',\n",
       " 'report',\n",
       " 'on',\n",
       " 'his',\n",
       " 'talks',\n",
       " 'with',\n",
       " 'Mr.',\n",
       " 'Macmillan',\n",
       " 'at',\n",
       " 'Chequers',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Macleod',\n",
       " 'went',\n",
       " 'on',\n",
       " 'with',\n",
       " 'the',\n",
       " 'conference',\n",
       " 'at',\n",
       " 'Lancaster',\n",
       " 'House',\n",
       " 'despite',\n",
       " 'the',\n",
       " 'crisis',\n",
       " 'which',\n",
       " 'had',\n",
       " 'blown',\n",
       " 'up',\n",
       " '.',\n",
       " 'He',\n",
       " 'has',\n",
       " 'now',\n",
       " 'revealed',\n",
       " 'his',\n",
       " 'full',\n",
       " 'plans',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Africans',\n",
       " 'and',\n",
       " 'Liberals',\n",
       " 'attending',\n",
       " '.',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out how many different words we have\n",
    "word_dict = {}\n",
    "for ent in word_l:\n",
    "    if ent in word_dict:\n",
    "        word_dict[ent] += 1.0\n",
    "    else:\n",
    "        word_dict[ent] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'a',\n",
       " 'in',\n",
       " 'was',\n",
       " 'that',\n",
       " 'is',\n",
       " 'he',\n",
       " 'for',\n",
       " 'I',\n",
       " 'with',\n",
       " 'had',\n",
       " 'be',\n",
       " 'his',\n",
       " 'it',\n",
       " 'as',\n",
       " 'on',\n",
       " 'The',\n",
       " 'at',\n",
       " 'by',\n",
       " 'not',\n",
       " 'which',\n",
       " 'have',\n",
       " 'but']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict['a']\n",
    "#ok we're going to pick the most common 50 words\n",
    "#were gonna get rid of punctuation\n",
    "\n",
    "common_words = sorted(word_dict, key=word_dict.get, reverse=True)[:30]\n",
    "\n",
    "common_word_no_punc = []\n",
    "for word in common_words:\n",
    "    if word not in string.punctuation:\n",
    "        common_word_no_punc.append(word)\n",
    "        \n",
    "common_word_no_punc\n",
    "num_words = len(common_word_no_punc)\n",
    "common_word_no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-' in string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 166)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_l\n",
    "img = imageio.imread('/Users/arushigupta/Desktop/IAM/words/a01/a01-000u/a01-000u-00-03.png')\n",
    "np.shape(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 176)\n"
     ]
    }
   ],
   "source": [
    "img2 = np.pad(img, [(2, 3), (5,5)], mode = \"constant\", constant_values = 0)\n",
    "print(np.shape(img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j = 0\n",
    "\n",
    "data_words = []\n",
    "data_locs = []\n",
    "\n",
    "\n",
    "\n",
    "for imp in loc_l:\n",
    "    if imp != '/Users/arushigupta/Desktop/IAM/words/a01/a01-117/a01-117-05-02.png' and imp != '/Users/arushigupta/Desktop/IAM/words/a01/a01-117/a01-117-00-02.png' and imp != '/Users/arushigupta/Desktop/IAM/words/r06/r06-022/r06-022-03-05.png':\n",
    "        if word_l[j] in common_word_no_punc:\n",
    "            data_words.append(word_l[j])\n",
    "            data_locs.append(imp)\n",
    "        \n",
    "    j += 1        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "1934\n"
     ]
    }
   ],
   "source": [
    "#actually we're just gonna find max width and heigth for the words we choose \n",
    "\n",
    "max_width = 0\n",
    "max_height = 0\n",
    "j = 0\n",
    "\n",
    "\n",
    "for imp in data_locs:\n",
    "    \n",
    "    img = misc.imread(imp)\n",
    "    sh = np.shape(img)\n",
    "    wi = sh[0]\n",
    "    hi = sh[1]\n",
    "    if wi > max_width:\n",
    "        max_width = wi\n",
    "    if hi > max_height:    \n",
    "        max_height = hi\n",
    "    j += 1        \n",
    "        \n",
    "        \n",
    "print(max_width)\n",
    "print(max_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#pad the images to max height and width\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.layers import Input, Conv2D, Dense, Activation, BatchNormalization, merge, AveragePooling2D, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras import Model\n",
    "from keras.datasets import cifar10, mnist\n",
    "from random import shuffle\n",
    "from math import floor, ceil\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ResNet50(include_top = True, weights = None, input_shape = (max_width, max_height, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 231, 1934, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 116, 967, 64) 3200        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 116, 967, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 116, 967, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 57, 483, 64)  0           activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 57, 483, 64)  4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 57, 483, 64)  256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 57, 483, 64)  0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 57, 483, 64)  36928       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 57, 483, 64)  256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 57, 483, 64)  0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 57, 483, 256) 16640       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 57, 483, 256) 16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 57, 483, 256) 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 57, 483, 256) 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 57, 483, 256) 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 57, 483, 256) 0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 57, 483, 64)  16448       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 57, 483, 64)  256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 57, 483, 64)  0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 57, 483, 64)  36928       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 57, 483, 64)  256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 57, 483, 64)  0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 57, 483, 256) 16640       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 57, 483, 256) 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 57, 483, 256) 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 57, 483, 256) 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 57, 483, 64)  16448       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 57, 483, 64)  256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 57, 483, 64)  0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 57, 483, 64)  36928       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 57, 483, 64)  256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 57, 483, 64)  0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 57, 483, 256) 16640       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 57, 483, 256) 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 57, 483, 256) 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 57, 483, 256) 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 29, 242, 128) 32896       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 29, 242, 128) 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 29, 242, 128) 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 29, 242, 128) 147584      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 29, 242, 128) 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 29, 242, 128) 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 29, 242, 512) 66048       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 29, 242, 512) 131584      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 29, 242, 512) 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 29, 242, 512) 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 29, 242, 512) 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 29, 242, 512) 0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 29, 242, 128) 65664       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 29, 242, 128) 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 29, 242, 128) 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 29, 242, 128) 147584      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 29, 242, 128) 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 29, 242, 128) 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 29, 242, 512) 66048       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 29, 242, 512) 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 29, 242, 512) 0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 29, 242, 512) 0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 29, 242, 128) 65664       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 29, 242, 128) 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 29, 242, 128) 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 29, 242, 128) 147584      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 29, 242, 128) 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 29, 242, 128) 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 29, 242, 512) 66048       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 29, 242, 512) 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 29, 242, 512) 0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 29, 242, 512) 0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 29, 242, 128) 65664       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 29, 242, 128) 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 29, 242, 128) 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 29, 242, 128) 147584      activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 29, 242, 128) 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 29, 242, 128) 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 29, 242, 512) 66048       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 29, 242, 512) 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 29, 242, 512) 0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 29, 242, 512) 0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 15, 121, 256) 131328      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 15, 121, 256) 0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 15, 121, 256) 0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 15, 121, 1024 525312      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 15, 121, 1024 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 15, 121, 1024 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 15, 121, 1024 0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 15, 121, 256) 0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 15, 121, 256) 0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 15, 121, 1024 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 15, 121, 1024 0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 15, 121, 256) 0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 15, 121, 256) 0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 15, 121, 1024 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 15, 121, 1024 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 15, 121, 256) 0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 15, 121, 256) 0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 15, 121, 1024 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 15, 121, 1024 0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 15, 121, 256) 0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 15, 121, 256) 0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 15, 121, 1024 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 15, 121, 1024 0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 15, 121, 256) 0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 15, 121, 256) 0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 15, 121, 1024 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 15, 121, 1024 0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 8, 61, 512)   524800      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 8, 61, 512)   2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 8, 61, 512)   0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 8, 61, 512)   2359808     activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 8, 61, 512)   2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 8, 61, 512)   0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 8, 61, 2048)  1050624     activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 8, 61, 2048)  2099200     activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 8, 61, 2048)  8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 8, 61, 2048)  8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 8, 61, 2048)  0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 8, 61, 2048)  0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 8, 61, 512)   1049088     activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 8, 61, 512)   2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 8, 61, 512)   0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 8, 61, 512)   2359808     activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 8, 61, 512)   2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 8, 61, 512)   0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 8, 61, 2048)  1050624     activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 8, 61, 2048)  8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 8, 61, 2048)  0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 8, 61, 2048)  0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 8, 61, 512)   1049088     activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 8, 61, 512)   2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 8, 61, 512)   0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 8, 61, 512)   2359808     activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 8, 61, 512)   2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 8, 61, 512)   0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 8, 61, 2048)  1050624     activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 8, 61, 2048)  8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 8, 61, 2048)  0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 8, 61, 2048)  0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 8, 2048)   0           activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 16384)        0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 1000)         16385000    flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 39,966,440\n",
      "Trainable params: 39,913,320\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 231, 1934, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 116, 967, 64) 3200        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 116, 967, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 116, 967, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 57, 483, 64)  0           activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 57, 483, 64)  4160        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 57, 483, 64)  256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 57, 483, 64)  0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 57, 483, 64)  36928       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 57, 483, 64)  256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 57, 483, 64)  0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 57, 483, 256) 16640       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 57, 483, 256) 16640       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 57, 483, 256) 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 57, 483, 256) 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 57, 483, 256) 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 57, 483, 256) 0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 57, 483, 64)  16448       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 57, 483, 64)  256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 57, 483, 64)  0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 57, 483, 64)  36928       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 57, 483, 64)  256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 57, 483, 64)  0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 57, 483, 256) 16640       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 57, 483, 256) 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 57, 483, 256) 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 57, 483, 256) 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 57, 483, 64)  16448       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 57, 483, 64)  256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 57, 483, 64)  0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 57, 483, 64)  36928       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 57, 483, 64)  256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 57, 483, 64)  0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 57, 483, 256) 16640       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 57, 483, 256) 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 57, 483, 256) 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 57, 483, 256) 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 29, 242, 128) 32896       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 29, 242, 128) 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 29, 242, 128) 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 29, 242, 128) 147584      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 29, 242, 128) 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 29, 242, 128) 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 29, 242, 512) 66048       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 29, 242, 512) 131584      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 29, 242, 512) 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 29, 242, 512) 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 29, 242, 512) 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 29, 242, 512) 0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 29, 242, 128) 65664       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 29, 242, 128) 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 29, 242, 128) 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 29, 242, 128) 147584      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 29, 242, 128) 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 29, 242, 128) 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 29, 242, 512) 66048       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 29, 242, 512) 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 29, 242, 512) 0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 29, 242, 512) 0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 29, 242, 128) 65664       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 29, 242, 128) 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 29, 242, 128) 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 29, 242, 128) 147584      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 29, 242, 128) 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 29, 242, 128) 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 29, 242, 512) 66048       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 29, 242, 512) 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 29, 242, 512) 0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 29, 242, 512) 0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 29, 242, 128) 65664       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 29, 242, 128) 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 29, 242, 128) 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 29, 242, 128) 147584      activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 29, 242, 128) 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 29, 242, 128) 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 29, 242, 512) 66048       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 29, 242, 512) 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 29, 242, 512) 0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 29, 242, 512) 0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 15, 121, 256) 131328      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 15, 121, 256) 0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 15, 121, 256) 0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 15, 121, 1024 525312      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 15, 121, 1024 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 15, 121, 1024 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 15, 121, 1024 0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 15, 121, 256) 0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 15, 121, 256) 0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 15, 121, 1024 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 15, 121, 1024 0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 15, 121, 256) 0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 15, 121, 256) 0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 15, 121, 1024 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 15, 121, 1024 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 15, 121, 256) 0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 15, 121, 256) 0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 15, 121, 1024 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 15, 121, 1024 0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 15, 121, 256) 0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 15, 121, 256) 0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 15, 121, 1024 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 15, 121, 1024 0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 15, 121, 256) 262400      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 15, 121, 256) 1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 15, 121, 256) 0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 15, 121, 256) 590080      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 15, 121, 256) 1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 15, 121, 256) 0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 15, 121, 1024 263168      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 15, 121, 1024 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 15, 121, 1024 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 15, 121, 1024 0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 8, 61, 512)   524800      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 8, 61, 512)   2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 8, 61, 512)   0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 8, 61, 512)   2359808     activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 8, 61, 512)   2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 8, 61, 512)   0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 8, 61, 2048)  1050624     activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 8, 61, 2048)  2099200     activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 8, 61, 2048)  8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 8, 61, 2048)  8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 8, 61, 2048)  0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 8, 61, 2048)  0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 8, 61, 512)   1049088     activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 8, 61, 512)   2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 8, 61, 512)   0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 8, 61, 512)   2359808     activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 8, 61, 512)   2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 8, 61, 512)   0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 8, 61, 2048)  1050624     activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 8, 61, 2048)  8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 8, 61, 2048)  0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 8, 61, 2048)  0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 8, 61, 512)   1049088     activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 8, 61, 512)   2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 8, 61, 512)   0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 8, 61, 512)   2359808     activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 8, 61, 512)   2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 8, 61, 512)   0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 8, 61, 2048)  1050624     activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 8, 61, 2048)  8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 8, 61, 2048)  0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 8, 61, 2048)  0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 8, 2048)   0           activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 16384)        0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 1000)         16385000    flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 26)           26026       fc1000[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 26)           0           dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 39,992,466\n",
      "Trainable params: 39,939,346\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "top_model = Dense(num_words)\n",
    "top_modelII = Activation('softmax')\n",
    "#top_model.add(Dense(num_words))\n",
    "model = Model(inputs=mod.input, outputs=top_model(mod.output))\n",
    "modelII = Model(inputs=model.input, outputs=top_modelII(model.output))\n",
    "print(modelII.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 0,\n",
       " 'The': 1,\n",
       " 'a': 2,\n",
       " 'and': 3,\n",
       " 'as': 4,\n",
       " 'at': 5,\n",
       " 'be': 6,\n",
       " 'but': 7,\n",
       " 'by': 8,\n",
       " 'for': 9,\n",
       " 'had': 10,\n",
       " 'have': 11,\n",
       " 'he': 12,\n",
       " 'his': 13,\n",
       " 'in': 14,\n",
       " 'is': 15,\n",
       " 'it': 16,\n",
       " 'not': 17,\n",
       " 'of': 18,\n",
       " 'on': 19,\n",
       " 'that': 20,\n",
       " 'the': 21,\n",
       " 'to': 22,\n",
       " 'was': 23,\n",
       " 'which': 24,\n",
       " 'with': 25}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#our labels are gonna be one-hot\n",
    "sorted(common_word_no_punc)\n",
    "hot_ind = {}\n",
    "k = 0\n",
    "for word in sorted(common_word_no_punc):\n",
    "    hot_ind[word] = k \n",
    "    k += 1\n",
    "    \n",
    "hot_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok so modelII is our model\n",
    "#now we pad the data and start training\n",
    "#if we can load all the data into memory we can use the keras image generator....\n",
    "\n",
    "\n",
    "#shuffle the data\n",
    "\n",
    "def my_batch_generator(batch_size, data_words, data_locs):\n",
    "    #print(data_words)\n",
    "    l = len(data_words)\n",
    "    print(l)\n",
    "    inds = np.arange(l)\n",
    "    np.random.shuffle(inds)\n",
    "    data_words = np.reshape( data_words, (l,1))\n",
    "    data_locs = np.reshape( data_locs, (l,1))\n",
    "    data_words = data_words[inds]\n",
    "    data_locs = data_locs[inds]\n",
    "    i = 0\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    while True:\n",
    "        #load the image\n",
    "        if i > l-1:\n",
    "            i = 0\n",
    "            inds = np.arange(l)\n",
    "            np.random.shuffle(inds)\n",
    "            data_words = np.reshape( data_words, (l,1))\n",
    "            data_locs = np.reshape( data_locs, (l,1))\n",
    "            data_words = data_words[inds]\n",
    "            data_locs = data_locs[inds]\n",
    "            \n",
    "        path = data_locs[i][0]\n",
    "        #print(path)\n",
    "        img = misc.imread(path)\n",
    "        lab = data_words[i][0]\n",
    "        im_w = np.shape(img)[0]\n",
    "        im_h = np.shape(img)[1]\n",
    "        pad_0_l = ceil((max_width - im_w)/2.0)\n",
    "        pad_0_r = floor((max_width - im_w)/2.0)\n",
    "        pad_1_l  = ceil((max_height - im_h)/2.0)\n",
    "        pad_1_r = floor((max_height - im_h)/2.0)\n",
    "        padded_img =  np.pad(img, [(pad_0_l, pad_0_r),(pad_1_l, pad_1_r)], mode = 'constant', constant_values = 0)\n",
    "        hot_lab = np.zeros(( len(hot_ind.keys()) , ))\n",
    "        labl = hot_ind[lab]\n",
    "        #print(labl)\n",
    "        hot_lab[labl] = 1\n",
    "        X_train.append(padded_img)\n",
    "        y_train.append(hot_lab)\n",
    "        i +=1\n",
    "        if i % batch_size == 0:\n",
    "            X_train = np.reshape(X_train, (batch_size, max_width, max_height, 1))\n",
    "            y_train = np.reshape(y_train, (batch_size, len(hot_ind.keys())))\n",
    "            yield X_train, y_train\n",
    "            X_train = []\n",
    "            y_train = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/arushigupta/Desktop/IAM/words/a01/a01-000u/a01-000u-00-02.png'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_locs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    }
   ],
   "source": [
    "l = len(data_locs)\n",
    "data_words = np.reshape( data_words, (l,1))\n",
    "data_locs = np.reshape( data_locs, (l,1))\n",
    "#split up data words and locs\n",
    "num_samp = len(data_locs)\n",
    "indices = np.random.choice(num_samp, ceil(.7*num_samp))\n",
    "test_ind = [x for x in range(0, num_samp) if x not in indices]\n",
    "train_words = np.load('/Users/arushigupta/Desktop/train_words.npy')\n",
    "train_ys =np.load('/Users/arushigupta/Desktop/train_ys.npy')\n",
    "data_gen = my_batch_generator(32, train_words, train_ys)\n",
    "xt, yt = next(data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['it'],\n",
       "       ['I'],\n",
       "       ['a'],\n",
       "       ...,\n",
       "       ['the'],\n",
       "       ['to'],\n",
       "       ['a']], dtype='<U5')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('/Users/arushigupta/Desktop/train_words.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (1, 1), strides=(1, 1))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), padding=\"same\")`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), strides=(1, 1))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/legacy/layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (1, 1), strides=(1, 1))`\n",
      "  from ipykernel import kernelapp as app\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), padding=\"same\")`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), strides=(2, 2))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\")`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), strides=(2, 2))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), strides=(1, 1))`\n",
      "  from ipykernel import kernelapp as app\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"same\")`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), strides=(2, 2))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\")`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), strides=(2, 2))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), strides=(1, 1))`\n",
      "  from ipykernel import kernelapp as app\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\")`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1))`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D(pool_size=(8, 8), padding=\"valid\")`\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:78: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return Activation('relu')(x)\n",
    "\n",
    "def neck(nip,nop,stride):\n",
    "    def unit(x):\n",
    "        nBottleneckPlane = int(nop / 4)\n",
    "        nbp = nBottleneckPlane\n",
    "\n",
    "        if nip==nop:\n",
    "            ident = x\n",
    "\n",
    "            #x = BatchNormalization(axis=-1)(x)\n",
    "            x = relu(x)\n",
    "            x = Conv2D(nbp,(1,1),\n",
    "            subsample=(stride,stride))(x)\n",
    "\n",
    "            #x = BatchNormalization(axis=-1)(x)\n",
    "            x = relu(x)\n",
    "            x = Conv2D(nbp,(3,3),border_mode='same')(x)\n",
    "\n",
    "            #x = BatchNormalization(axis=-1)(x)\n",
    "            x = relu(x)\n",
    "            x = Conv2D(nop,1,1)(x)\n",
    "\n",
    "            out = merge([ident,x],mode='sum')\n",
    "        else:\n",
    "            #x = BatchNormalization(axis=-1)(x)\n",
    "            x = relu(x)\n",
    "            ident = x\n",
    "\n",
    "            x = Conv2D(nbp,( 1,1),\n",
    "            subsample=(stride,stride))(x)\n",
    "\n",
    "            #x = BatchNormalization(axis=-1)(x)\n",
    "            x = relu(x)\n",
    "            x = Conv2D(nbp,(3,3) ,border_mode='same')(x)\n",
    "\n",
    "            #x = BatchNormalization(axis=-1)(x)\n",
    "            x = relu(x)\n",
    "            x = Conv2D(nop,1,1)(x)\n",
    "\n",
    "            ident = Conv2D(nop,1,1,\n",
    "            subsample=(stride,stride))(ident)\n",
    "\n",
    "            out = merge([ident,x],mode='sum')\n",
    "\n",
    "        return out\n",
    "    return unit\n",
    "\n",
    "def cake(nip,nop,layers,std):\n",
    "    def unit(x):\n",
    "        for i in range(layers):\n",
    "            if i==0:\n",
    "                x = neck(nip,nop,std)(x)\n",
    "            else:\n",
    "                x = neck(nop,nop,1)(x)\n",
    "        return x\n",
    "    return unit\n",
    "\n",
    "inp = Input(shape=(max_width,max_height,1))\n",
    "i = inp\n",
    "\n",
    "i = Conv2D(16, (3,3) , padding='same')(i)\n",
    "\n",
    "i = cake(16,32,3,1)(i) #32x32\n",
    "i = cake(32,64,3,2)(i) #16x16\n",
    "i = cake(64,128,3,2)(i) #8x8\n",
    "\n",
    "i = BatchNormalization(axis=-1)(i)\n",
    "i = relu(i)\n",
    "\n",
    "i = AveragePooling2D(pool_size=(8,8),border_mode='valid')(i) #1x1\n",
    "i = Flatten()(i) # 128\n",
    "\n",
    "i = Dense(len(common_word_no_punc))(i)\n",
    "i = Activation('softmax')(i)\n",
    "\n",
    "model = Model(input=inp,output=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 231, 1934, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 231, 1934, 16 160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 231, 1934, 16 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 231, 1934, 8) 136         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 231, 1934, 8) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 231, 1934, 8) 584         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 231, 1934, 8) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 231, 1934, 32 544         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 231, 1934, 32 288         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 231, 1934, 32 0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 231, 1934, 32 0           merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 231, 1934, 8) 264         activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 231, 1934, 8) 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 231, 1934, 8) 584         activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 231, 1934, 8) 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 231, 1934, 32 288         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 231, 1934, 32 0           merge_1[0][0]                    \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 231, 1934, 32 0           merge_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 231, 1934, 8) 264         activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 231, 1934, 8) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 231, 1934, 8) 584         activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 231, 1934, 8) 0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 231, 1934, 32 288         activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_3 (Merge)                 (None, 231, 1934, 32 0           merge_2[0][0]                    \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 231, 1934, 32 0           merge_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 116, 967, 16) 528         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 116, 967, 16) 0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 116, 967, 16) 2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 116, 967, 16) 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 116, 967, 64) 2112        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 116, 967, 64) 1088        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_4 (Merge)                 (None, 116, 967, 64) 0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 116, 967, 64) 0           merge_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 116, 967, 16) 1040        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 116, 967, 16) 0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 116, 967, 16) 2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 116, 967, 16) 0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 116, 967, 64) 1088        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_5 (Merge)                 (None, 116, 967, 64) 0           merge_4[0][0]                    \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 116, 967, 64) 0           merge_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 116, 967, 16) 1040        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 116, 967, 16) 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 116, 967, 16) 2320        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 116, 967, 16) 0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 116, 967, 64) 1088        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_6 (Merge)                 (None, 116, 967, 64) 0           merge_5[0][0]                    \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 116, 967, 64) 0           merge_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 58, 484, 32)  2080        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 58, 484, 32)  0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 58, 484, 32)  9248        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 58, 484, 32)  0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 58, 484, 128) 8320        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 58, 484, 128) 4224        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_7 (Merge)                 (None, 58, 484, 128) 0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 58, 484, 128) 0           merge_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 58, 484, 32)  4128        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 58, 484, 32)  0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 58, 484, 32)  9248        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 58, 484, 32)  0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 58, 484, 128) 4224        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_8 (Merge)                 (None, 58, 484, 128) 0           merge_7[0][0]                    \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 58, 484, 128) 0           merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 58, 484, 32)  4128        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 58, 484, 32)  0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 58, 484, 32)  9248        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 58, 484, 32)  0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 58, 484, 128) 4224        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "merge_9 (Merge)                 (None, 58, 484, 128) 0           merge_8[0][0]                    \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 58, 484, 128) 512         merge_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 58, 484, 128) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 7, 60, 128)   0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 53760)        0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 26)           1397786     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 26)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,476,298\n",
      "Trainable params: 1,476,042\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py:1269: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 60s 2s/step - loss: 0.8685 - acc: 0.6562\n",
      "1\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.5539 - acc: 0.5000\n",
      "2\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2086 - acc: 0.6562\n",
      "3\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6269 - acc: 0.5312\n",
      "4\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3995 - acc: 0.6875\n",
      "5\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2638 - acc: 0.7188\n",
      "6\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1969 - acc: 0.6250\n",
      "7\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0748 - acc: 0.7188\n",
      "8\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.7942 - acc: 0.5938\n",
      "9\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.1852 - acc: 0.6562\n",
      "10\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.1917 - acc: 0.6250\n",
      "11\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0961 - acc: 0.6250\n",
      "12\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2787 - acc: 0.6562\n",
      "13\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.3965 - acc: 0.5000\n",
      "14\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1860 - acc: 0.6875\n",
      "15\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0356 - acc: 0.6562\n",
      "16\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3857 - acc: 0.6250\n",
      "17\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.2044 - acc: 0.5938\n",
      "18\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1010 - acc: 0.6562\n",
      "19\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.9053 - acc: 0.7812\n",
      "20\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0373 - acc: 0.6875\n",
      "21\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4202 - acc: 0.5000\n",
      "22\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1080 - acc: 0.6875\n",
      "23\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1317 - acc: 0.5938\n",
      "24\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2942 - acc: 0.6562\n",
      "25\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9010 - acc: 0.6875\n",
      "26\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1367 - acc: 0.7188\n",
      "27\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8600 - acc: 0.7500\n",
      "28\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2181 - acc: 0.7500\n",
      "29\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6581 - acc: 0.4688\n",
      "30\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9945 - acc: 0.7500\n",
      "31\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2492 - acc: 0.6562\n",
      "32\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0551 - acc: 0.7188\n",
      "33\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.4744 - acc: 0.5938\n",
      "34\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0871 - acc: 0.6875\n",
      "35\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 2.0527 - acc: 0.3438\n",
      "36\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8775 - acc: 0.6875\n",
      "37\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2886 - acc: 0.6562\n",
      "38\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1797 - acc: 0.7188\n",
      "39\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1819 - acc: 0.6562\n",
      "40\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.7329 - acc: 0.5000\n",
      "41\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4857 - acc: 0.4688\n",
      "42\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0668 - acc: 0.6875\n",
      "43\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4950 - acc: 0.5938\n",
      "44\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2758 - acc: 0.5938\n",
      "45\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0818 - acc: 0.6562\n",
      "46\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0236 - acc: 0.7500\n",
      "47\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2768 - acc: 0.5312\n",
      "48\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9143 - acc: 0.7500\n",
      "49\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5093 - acc: 0.5312\n",
      "50\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2305 - acc: 0.6250\n",
      "51\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3140 - acc: 0.5625\n",
      "52\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1876 - acc: 0.6562\n",
      "53\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6076 - acc: 0.7188\n",
      "54\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3892 - acc: 0.6562\n",
      "55\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5382 - acc: 0.5312\n",
      "56\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1280 - acc: 0.6250\n",
      "57\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3055 - acc: 0.5000\n",
      "58\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0621 - acc: 0.6250\n",
      "59\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4559 - acc: 0.5312\n",
      "60\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 2.4925 - acc: 0.5000\n",
      "61\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4191 - acc: 0.5312\n",
      "62\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3507 - acc: 0.5938\n",
      "63\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.9637 - acc: 0.7188\n",
      "64\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0724 - acc: 0.6250\n",
      "65\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4766 - acc: 0.6875\n",
      "66\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0548 - acc: 0.7500\n",
      "67\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.4748 - acc: 0.6250\n",
      "68\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0922 - acc: 0.6562\n",
      "69\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1199 - acc: 0.7188\n",
      "70\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2243 - acc: 0.5938\n",
      "71\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7388 - acc: 0.8125\n",
      "72\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0873 - acc: 0.7188\n",
      "73\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7814 - acc: 0.7812\n",
      "74\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9557 - acc: 0.8125\n",
      "75\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0101 - acc: 0.7500\n",
      "76\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.7034 - acc: 0.5000\n",
      "77\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1178 - acc: 0.6250\n",
      "78\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1105 - acc: 0.7500\n",
      "79\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7698 - acc: 0.7500\n",
      "80\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3862 - acc: 0.5625\n",
      "81\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3440 - acc: 0.6875\n",
      "82\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7437 - acc: 0.7188\n",
      "83\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3226 - acc: 0.6250\n",
      "84\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9091 - acc: 0.6875\n",
      "85\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0116 - acc: 0.6875\n",
      "86\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4097 - acc: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3620 - acc: 0.7188\n",
      "88\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2410 - acc: 0.6250\n",
      "89\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8217 - acc: 0.8125\n",
      "90\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.4986 - acc: 0.5000\n",
      "91\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0964 - acc: 0.7188\n",
      "92\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.4738 - acc: 0.6562\n",
      "93\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9860 - acc: 0.6250\n",
      "94\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9180 - acc: 0.7500\n",
      "95\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3997 - acc: 0.6250\n",
      "96\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1615 - acc: 0.6562\n",
      "97\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.5260 - acc: 0.6250\n",
      "98\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3606 - acc: 0.6250\n",
      "99\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1404 - acc: 0.6875\n",
      "100\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9635 - acc: 0.6875\n",
      "101\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3342 - acc: 0.6875\n",
      "102\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1848 - acc: 0.7500\n",
      "103\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1119 - acc: 0.6875\n",
      "104\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2331 - acc: 0.6875\n",
      "105\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.7480 - acc: 0.4062\n",
      "106\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5433 - acc: 0.5938\n",
      "107\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0664 - acc: 0.7188\n",
      "108\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0068 - acc: 0.6875\n",
      "109\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9597 - acc: 0.7812\n",
      "110\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0780 - acc: 0.7188\n",
      "111\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5558 - acc: 0.5625\n",
      "112\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3440 - acc: 0.6250\n",
      "113\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 2.1436 - acc: 0.5312\n",
      "114\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3983 - acc: 0.5000\n",
      "115\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4398 - acc: 0.4375\n",
      "116\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2954 - acc: 0.5625\n",
      "117\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3924 - acc: 0.6875\n",
      "118\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9663 - acc: 0.6562\n",
      "119\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1138 - acc: 0.5938\n",
      "120\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1747 - acc: 0.5625\n",
      "121\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1656 - acc: 0.7500\n",
      "122\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1833 - acc: 0.7188\n",
      "123\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9875 - acc: 0.6875\n",
      "124\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4004 - acc: 0.4375\n",
      "125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2924 - acc: 0.6875\n",
      "126\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5495 - acc: 0.5625\n",
      "127\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2802 - acc: 0.6250\n",
      "128\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0255 - acc: 0.6875\n",
      "129\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1486 - acc: 0.6875\n",
      "130\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4460 - acc: 0.5938\n",
      "131\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1115 - acc: 0.7812\n",
      "132\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6427 - acc: 0.5625\n",
      "133\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9477 - acc: 0.7188\n",
      "134\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1554 - acc: 0.5625\n",
      "135\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4457 - acc: 0.5938\n",
      "136\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0399 - acc: 0.6875\n",
      "137\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3383 - acc: 0.5625\n",
      "138\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2938 - acc: 0.5625\n",
      "139\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.4194 - acc: 0.6250\n",
      "140\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9945 - acc: 0.6875\n",
      "141\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0536 - acc: 0.6562\n",
      "142\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2525 - acc: 0.6250\n",
      "143\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3171 - acc: 0.6562\n",
      "144\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3980 - acc: 0.5000\n",
      "145\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1404 - acc: 0.7188\n",
      "146\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0893 - acc: 0.6562\n",
      "147\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4549 - acc: 0.6562\n",
      "148\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9868 - acc: 0.6875\n",
      "149\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8329 - acc: 0.7500\n",
      "150\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0442 - acc: 0.7500\n",
      "151\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0140 - acc: 0.7188\n",
      "152\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7454 - acc: 0.7812\n",
      "153\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0295 - acc: 0.6875\n",
      "154\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2785 - acc: 0.6250\n",
      "155\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9201 - acc: 0.7500\n",
      "156\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.7284 - acc: 0.5625\n",
      "157\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2485 - acc: 0.6875\n",
      "158\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9289 - acc: 0.7188\n",
      "159\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.9292 - acc: 0.5312\n",
      "160\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8237 - acc: 0.8125\n",
      "161\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2205 - acc: 0.6562\n",
      "162\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8643 - acc: 0.7188\n",
      "163\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1162 - acc: 0.5312\n",
      "164\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1142 - acc: 0.6562\n",
      "165\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0011 - acc: 0.6875\n",
      "166\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1176 - acc: 0.6562\n",
      "167\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1810 - acc: 0.4688\n",
      "168\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9924 - acc: 0.7500\n",
      "169\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8941 - acc: 0.6875\n",
      "170\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0523 - acc: 0.6875\n",
      "171\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3572 - acc: 0.5938\n",
      "172\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1593 - acc: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2203 - acc: 0.5625\n",
      "174\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.4712 - acc: 0.5625\n",
      "175\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1298 - acc: 0.6250\n",
      "176\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1712 - acc: 0.6875\n",
      "177\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.5151 - acc: 0.6562\n",
      "178\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8800 - acc: 0.7188\n",
      "179\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5925 - acc: 0.6250\n",
      "180\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7250 - acc: 0.8125\n",
      "181\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0534 - acc: 0.7188\n",
      "182\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1237 - acc: 0.6250\n",
      "183\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3087 - acc: 0.5625\n",
      "184\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3364 - acc: 0.5938\n",
      "185\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9251 - acc: 0.6875\n",
      "186\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1877 - acc: 0.5312\n",
      "187\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3145 - acc: 0.5938\n",
      "188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8164 - acc: 0.7500\n",
      "189\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0494 - acc: 0.6875\n",
      "190\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3160 - acc: 0.6250\n",
      "191\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0703 - acc: 0.6875\n",
      "192\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 57s 2s/step - loss: 1.2251 - acc: 0.5938\n",
      "193\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0672 - acc: 0.6562\n",
      "194\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6349 - acc: 0.8438\n",
      "195\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5154 - acc: 0.5938\n",
      "196\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0445 - acc: 0.7500\n",
      "197\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9093 - acc: 0.7500\n",
      "198\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1288 - acc: 0.6250\n",
      "199\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0576 - acc: 0.6562\n",
      "200\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0015 - acc: 0.5938\n",
      "201\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0499 - acc: 0.7812\n",
      "202\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2677 - acc: 0.7500\n",
      "203\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1188 - acc: 0.6562\n",
      "204\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.4014 - acc: 0.5625\n",
      "205\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1331 - acc: 0.6250\n",
      "206\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1128 - acc: 0.5938\n",
      "207\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6622 - acc: 0.5938\n",
      "208\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1164 - acc: 0.6562\n",
      "209\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1957 - acc: 0.6562\n",
      "210\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0024 - acc: 0.6562\n",
      "211\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9149 - acc: 0.6875\n",
      "212\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1264 - acc: 0.6562\n",
      "213\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9528 - acc: 0.7500\n",
      "214\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8945 - acc: 0.6250\n",
      "215\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1892 - acc: 0.5625\n",
      "216\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0452 - acc: 0.6562\n",
      "217\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0589 - acc: 0.6875\n",
      "218\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9668 - acc: 0.7500\n",
      "219\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8866 - acc: 0.8125\n",
      "220\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1147 - acc: 0.7188\n",
      "221\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8633 - acc: 0.6875\n",
      "222\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4243 - acc: 0.6875\n",
      "223\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9609 - acc: 0.6562\n",
      "224\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0565 - acc: 0.6562\n",
      "225\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0679 - acc: 0.6875\n",
      "226\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8522 - acc: 0.8125\n",
      "227\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0010 - acc: 0.7812\n",
      "228\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0116 - acc: 0.7500\n",
      "229\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8070 - acc: 0.6875\n",
      "230\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9823 - acc: 0.6562\n",
      "231\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9562 - acc: 0.7500\n",
      "232\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1232 - acc: 0.7188\n",
      "233\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8152 - acc: 0.7188\n",
      "234\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2482 - acc: 0.6250\n",
      "235\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0161 - acc: 0.7812\n",
      "236\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9522 - acc: 0.6875\n",
      "237\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2029 - acc: 0.5625\n",
      "238\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3355 - acc: 0.5625\n",
      "239\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.7520 - acc: 0.4688\n",
      "240\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2609 - acc: 0.6250\n",
      "241\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3338 - acc: 0.5000\n",
      "242\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8226 - acc: 0.7188\n",
      "243\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8439 - acc: 0.7188\n",
      "244\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0231 - acc: 0.7500\n",
      "245\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9670 - acc: 0.7500\n",
      "246\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9508 - acc: 0.7812\n",
      "247\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4834 - acc: 0.4688\n",
      "248\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1342 - acc: 0.6562\n",
      "249\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9713 - acc: 0.6875\n",
      "250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2000 - acc: 0.5312\n",
      "251\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8505 - acc: 0.7812\n",
      "252\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.5479 - acc: 0.5938\n",
      "253\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2959 - acc: 0.6250\n",
      "254\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9359 - acc: 0.7188\n",
      "255\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9543 - acc: 0.7812\n",
      "256\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7430 - acc: 0.8125\n",
      "257\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6936 - acc: 0.8125\n",
      "258\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2374 - acc: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7114 - acc: 0.7812\n",
      "260\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.4048 - acc: 0.5312\n",
      "261\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0486 - acc: 0.6562\n",
      "262\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0373 - acc: 0.6875\n",
      "263\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3043 - acc: 0.7188\n",
      "264\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8276 - acc: 0.7812\n",
      "265\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0671 - acc: 0.6250\n",
      "266\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1799 - acc: 0.6562\n",
      "267\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9946 - acc: 0.6875\n",
      "268\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1838 - acc: 0.6250\n",
      "269\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9730 - acc: 0.6875\n",
      "270\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2417 - acc: 0.7812\n",
      "271\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2405 - acc: 0.7500\n",
      "272\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7038 - acc: 0.7500\n",
      "273\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9759 - acc: 0.7188\n",
      "274\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6912 - acc: 0.7500\n",
      "275\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8635 - acc: 0.6875\n",
      "276\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3832 - acc: 0.6875\n",
      "277\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.5791 - acc: 0.5625\n",
      "278\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0448 - acc: 0.5938\n",
      "279\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9434 - acc: 0.6562\n",
      "280\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0622 - acc: 0.7188\n",
      "281\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.4851 - acc: 0.7188\n",
      "282\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7416 - acc: 0.7812\n",
      "283\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8381 - acc: 0.7500\n",
      "284\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8404 - acc: 0.7188\n",
      "285\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2297 - acc: 0.6562\n",
      "286\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7194 - acc: 0.8125\n",
      "287\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0110 - acc: 0.6875\n",
      "288\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9409 - acc: 0.7500\n",
      "289\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8410 - acc: 0.7500\n",
      "290\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 57s 2s/step - loss: 0.8577 - acc: 0.7500\n",
      "291\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3895 - acc: 0.6562\n",
      "292\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.2822 - acc: 0.7188\n",
      "293\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9606 - acc: 0.7500\n",
      "294\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.9604 - acc: 0.7188\n",
      "295\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.5412 - acc: 0.5000\n",
      "296\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8252 - acc: 0.7500\n",
      "297\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 53s 2s/step - loss: 1.4419 - acc: 0.7188\n",
      "298\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.3498 - acc: 0.5938\n",
      "299\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.0833 - acc: 0.6875\n",
      "300\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.1210 - acc: 0.6562\n",
      "301\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.7650 - acc: 0.7812\n",
      "302\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.2517 - acc: 0.7500\n",
      "303\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8670 - acc: 0.6250\n",
      "304\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.7104 - acc: 0.7812\n",
      "305\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.3135 - acc: 0.5625\n",
      "306\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.0363 - acc: 0.7812\n",
      "307\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.7993 - acc: 0.7500\n",
      "308\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8702 - acc: 0.6875\n",
      "309\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2498 - acc: 0.6875\n",
      "310\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8685 - acc: 0.7812\n",
      "311\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0765 - acc: 0.6562\n",
      "312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5908 - acc: 0.8125\n",
      "313\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.1551 - acc: 0.6250\n",
      "314\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6427 - acc: 0.5625\n",
      "315\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.7957 - acc: 0.5000\n",
      "316\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2528 - acc: 0.7188\n",
      "317\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.9183 - acc: 0.5312\n",
      "318\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.2876 - acc: 0.6875\n",
      "319\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9851 - acc: 0.6875\n",
      "320\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5859 - acc: 0.7812\n",
      "321\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.2913 - acc: 0.6562\n",
      "322\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1442 - acc: 0.6562\n",
      "323\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0252 - acc: 0.6875\n",
      "324\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.1217 - acc: 0.5938\n",
      "325\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7922 - acc: 0.7500\n",
      "326\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6791 - acc: 0.5312\n",
      "327\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1579 - acc: 0.6562\n",
      "328\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8996 - acc: 0.7188\n",
      "329\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.0838 - acc: 0.7500\n",
      "330\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4085 - acc: 0.6250\n",
      "331\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1507 - acc: 0.6875\n",
      "332\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0699 - acc: 0.6875\n",
      "333\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0777 - acc: 0.8125\n",
      "334\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8461 - acc: 0.8125\n",
      "335\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8364 - acc: 0.7188\n",
      "336\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8601 - acc: 0.6875\n",
      "337\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8113 - acc: 0.7812\n",
      "338\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5575 - acc: 0.8125\n",
      "339\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5685 - acc: 0.8125\n",
      "340\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8001 - acc: 0.6562\n",
      "341\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2104 - acc: 0.5312\n",
      "342\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8943 - acc: 0.6875\n",
      "343\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3076 - acc: 0.5000\n",
      "344\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5220 - acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2190 - acc: 0.6562\n",
      "346\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.0689 - acc: 0.6250\n",
      "347\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2762 - acc: 0.6250\n",
      "348\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0001 - acc: 0.6875\n",
      "349\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.1791 - acc: 0.6250\n",
      "350\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0566 - acc: 0.6562\n",
      "351\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8476 - acc: 0.8125\n",
      "352\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0793 - acc: 0.6875\n",
      "353\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9343 - acc: 0.7500\n",
      "354\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0011 - acc: 0.7188\n",
      "355\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3327 - acc: 0.7500\n",
      "356\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6429 - acc: 0.8438\n",
      "357\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7582 - acc: 0.8125\n",
      "358\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1245 - acc: 0.7188\n",
      "359\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8004 - acc: 0.7188\n",
      "360\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7505 - acc: 0.6250\n",
      "361\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9687 - acc: 0.6250\n",
      "362\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8558 - acc: 0.8125\n",
      "363\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0294 - acc: 0.6562\n",
      "364\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5085 - acc: 0.6562\n",
      "365\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0248 - acc: 0.6250\n",
      "366\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9566 - acc: 0.6250\n",
      "367\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0312 - acc: 0.7500\n",
      "368\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7135 - acc: 0.7812\n",
      "369\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1558 - acc: 0.6875\n",
      "370\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0327 - acc: 0.8125\n",
      "371\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1331 - acc: 0.7188\n",
      "372\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6573 - acc: 0.7812\n",
      "373\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6851 - acc: 0.8125\n",
      "374\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3167 - acc: 0.7812\n",
      "375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7850 - acc: 0.7500\n",
      "376\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.1329 - acc: 0.5938\n",
      "377\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1366 - acc: 0.7812\n",
      "378\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9850 - acc: 0.6562\n",
      "379\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7706 - acc: 0.8125\n",
      "380\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8774 - acc: 0.6875\n",
      "381\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0394 - acc: 0.7188\n",
      "382\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5249 - acc: 0.5625\n",
      "383\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8207 - acc: 0.7812\n",
      "384\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1562 - acc: 0.7500\n",
      "385\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1355 - acc: 0.6875\n",
      "386\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7091 - acc: 0.8438\n",
      "387\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0372 - acc: 0.7812\n",
      "388\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8688 - acc: 0.7188\n",
      "389\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3226 - acc: 0.6250\n",
      "390\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9930 - acc: 0.6875\n",
      "391\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.5101 - acc: 0.5938\n",
      "392\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3406 - acc: 0.6875\n",
      "393\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.7621 - acc: 0.8125\n",
      "394\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0086 - acc: 0.6875\n",
      "395\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0252 - acc: 0.7188\n",
      "396\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.9998 - acc: 0.6562\n",
      "397\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2527 - acc: 0.6875\n",
      "398\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8416 - acc: 0.7812\n",
      "399\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9541 - acc: 0.7500\n",
      "400\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6975 - acc: 0.8125\n",
      "401\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0279 - acc: 0.7188\n",
      "402\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8231 - acc: 0.6875\n",
      "403\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9074 - acc: 0.7188\n",
      "404\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9881 - acc: 0.6875\n",
      "405\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0803 - acc: 0.6562\n",
      "406\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9993 - acc: 0.6562\n",
      "407\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1871 - acc: 0.6562\n",
      "408\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0654 - acc: 0.6875\n",
      "409\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7685 - acc: 0.7188\n",
      "410\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7139 - acc: 0.7812\n",
      "411\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1771 - acc: 0.5938\n",
      "412\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3328 - acc: 0.5938\n",
      "413\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0380 - acc: 0.6250\n",
      "414\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1287 - acc: 0.6562\n",
      "415\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0762 - acc: 0.6875\n",
      "416\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8945 - acc: 0.7188\n",
      "417\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0196 - acc: 0.7188\n",
      "418\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3533 - acc: 0.6875\n",
      "419\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3194 - acc: 0.5938\n",
      "420\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7923 - acc: 0.7812\n",
      "421\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4486 - acc: 0.6562\n",
      "422\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6702 - acc: 0.6875\n",
      "423\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4613 - acc: 0.7188\n",
      "424\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8333 - acc: 0.7500\n",
      "425\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4831 - acc: 0.6250\n",
      "426\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7753 - acc: 0.7188\n",
      "427\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3645 - acc: 0.7188\n",
      "428\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7107 - acc: 0.7500\n",
      "429\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2100 - acc: 0.6562\n",
      "430\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4445 - acc: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0404 - acc: 0.7500\n",
      "432\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7135 - acc: 0.8750\n",
      "433\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8488 - acc: 0.7812\n",
      "434\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8549 - acc: 0.7500\n",
      "435\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0721 - acc: 0.5938\n",
      "436\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3613 - acc: 0.5312\n",
      "437\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3165 - acc: 0.5312\n",
      "438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9019 - acc: 0.6875\n",
      "439\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 1.2969 - acc: 0.8125\n",
      "440\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9336 - acc: 0.8125\n",
      "441\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9525 - acc: 0.7500\n",
      "442\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0544 - acc: 0.7812\n",
      "443\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6418 - acc: 0.8125\n",
      "444\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9380 - acc: 0.6875\n",
      "445\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1448 - acc: 0.6875\n",
      "446\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.7221 - acc: 0.5625\n",
      "447\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.6549 - acc: 0.8125\n",
      "448\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7023 - acc: 0.7812\n",
      "449\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9316 - acc: 0.7812\n",
      "450\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0198 - acc: 0.6250\n",
      "451\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2290 - acc: 0.7500\n",
      "452\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9508 - acc: 0.7188\n",
      "453\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3560 - acc: 0.5938\n",
      "454\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9538 - acc: 0.7812\n",
      "455\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9997 - acc: 0.7188\n",
      "456\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.6076 - acc: 0.8125\n",
      "457\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8916 - acc: 0.7188\n",
      "458\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9883 - acc: 0.7188\n",
      "459\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3013 - acc: 0.7812\n",
      "460\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9028 - acc: 0.7500\n",
      "461\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7503 - acc: 0.8125\n",
      "462\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7599 - acc: 0.8125\n",
      "463\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4144 - acc: 0.7188\n",
      "464\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8742 - acc: 0.7188\n",
      "465\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0269 - acc: 0.6250\n",
      "466\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0560 - acc: 0.6875\n",
      "467\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3260 - acc: 0.4688\n",
      "468\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7875 - acc: 0.6562\n",
      "469\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0463 - acc: 0.6875\n",
      "470\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8686 - acc: 0.6250\n",
      "471\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0396 - acc: 0.6562\n",
      "472\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6175 - acc: 0.5938\n",
      "473\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5463 - acc: 0.8125\n",
      "474\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6816 - acc: 0.8125\n",
      "475\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6832 - acc: 0.6250\n",
      "476\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7774 - acc: 0.7500\n",
      "477\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8092 - acc: 0.7188\n",
      "478\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9839 - acc: 0.5938\n",
      "479\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8614 - acc: 0.6875\n",
      "480\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.8126 - acc: 0.7500\n",
      "481\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0216 - acc: 0.6875\n",
      "482\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6873 - acc: 0.6562\n",
      "483\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9020 - acc: 0.6875\n",
      "484\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9753 - acc: 0.6562\n",
      "485\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8511 - acc: 0.7500\n",
      "486\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2372 - acc: 0.6250\n",
      "487\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0221 - acc: 0.7188\n",
      "488\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6632 - acc: 0.8438\n",
      "489\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4648 - acc: 0.7188\n",
      "490\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6443 - acc: 0.8750\n",
      "491\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8462 - acc: 0.7188\n",
      "492\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3145 - acc: 0.5625\n",
      "493\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0888 - acc: 0.7188\n",
      "494\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9092 - acc: 0.6562\n",
      "495\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7417 - acc: 0.7500\n",
      "496\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7935 - acc: 0.8125\n",
      "497\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0187 - acc: 0.6562\n",
      "498\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3204 - acc: 0.7500\n",
      "499\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7442 - acc: 0.7500\n",
      "500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9571 - acc: 0.7500\n",
      "501\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1553 - acc: 0.7812\n",
      "502\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5362 - acc: 0.9062\n",
      "503\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9598 - acc: 0.7812\n",
      "504\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7653 - acc: 0.7500\n",
      "505\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7701 - acc: 0.7188\n",
      "506\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6087 - acc: 0.8438\n",
      "507\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6933 - acc: 0.7812\n",
      "508\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9011 - acc: 0.7812\n",
      "509\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9048 - acc: 0.7812\n",
      "510\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6085 - acc: 0.8125\n",
      "511\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8150 - acc: 0.8438\n",
      "512\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7575 - acc: 0.8125\n",
      "513\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1468 - acc: 0.6562\n",
      "514\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6821 - acc: 0.7500\n",
      "515\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2971 - acc: 0.7188\n",
      "516\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6807 - acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9563 - acc: 0.6875\n",
      "518\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1345 - acc: 0.7500\n",
      "519\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2492 - acc: 0.7188\n",
      "520\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9680 - acc: 0.7500\n",
      "521\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6983 - acc: 0.8438\n",
      "522\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6249 - acc: 0.7812\n",
      "523\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9450 - acc: 0.8125\n",
      "524\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6970 - acc: 0.9062\n",
      "525\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1070 - acc: 0.7188\n",
      "526\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.7541 - acc: 0.4375\n",
      "527\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7532 - acc: 0.9062\n",
      "528\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3132 - acc: 0.7812\n",
      "529\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5768 - acc: 0.7500\n",
      "530\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7597 - acc: 0.7812\n",
      "531\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0326 - acc: 0.7188\n",
      "532\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9309 - acc: 0.7188\n",
      "533\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9101 - acc: 0.7812\n",
      "534\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8491 - acc: 0.7500\n",
      "535\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9698 - acc: 0.7500\n",
      "536\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7341 - acc: 0.7812\n",
      "537\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.6071 - acc: 0.7500\n",
      "538\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2403 - acc: 0.7500\n",
      "539\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9546 - acc: 0.6875\n",
      "540\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1833 - acc: 0.6875\n",
      "541\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8984 - acc: 0.7812\n",
      "542\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0356 - acc: 0.7812\n",
      "543\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9154 - acc: 0.7188\n",
      "544\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7862 - acc: 0.7188\n",
      "545\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9612 - acc: 0.6562\n",
      "546\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7034 - acc: 0.7812\n",
      "547\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8733 - acc: 0.7812\n",
      "548\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1150 - acc: 0.5938\n",
      "549\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8919 - acc: 0.7812\n",
      "550\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8221 - acc: 0.7812\n",
      "551\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7393 - acc: 0.7500\n",
      "552\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6973 - acc: 0.8125\n",
      "553\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0377 - acc: 0.5938\n",
      "554\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9670 - acc: 0.6875\n",
      "555\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6502 - acc: 0.8438\n",
      "556\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0896 - acc: 0.7188\n",
      "557\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7621 - acc: 0.7812\n",
      "558\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9014 - acc: 0.7500\n",
      "559\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5647 - acc: 0.8750\n",
      "560\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2232 - acc: 0.6250\n",
      "561\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1455 - acc: 0.6250\n",
      "562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6591 - acc: 0.8125\n",
      "563\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8716 - acc: 0.7188\n",
      "564\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9475 - acc: 0.6875\n",
      "565\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2366 - acc: 0.5938\n",
      "566\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4478 - acc: 0.5938\n",
      "567\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0175 - acc: 0.7188\n",
      "568\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6964 - acc: 0.7812\n",
      "569\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6905 - acc: 0.8125\n",
      "570\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8161 - acc: 0.6875\n",
      "571\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1212 - acc: 0.8438\n",
      "572\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7965 - acc: 0.8438\n",
      "573\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6511 - acc: 0.7812\n",
      "574\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9085 - acc: 0.7500\n",
      "575\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8939 - acc: 0.6875\n",
      "576\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8647 - acc: 0.8125\n",
      "577\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5740 - acc: 0.8750\n",
      "578\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7899 - acc: 0.8438\n",
      "579\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6568 - acc: 0.7188\n",
      "580\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7962 - acc: 0.7188\n",
      "581\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8861 - acc: 0.7812\n",
      "582\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7208 - acc: 0.7500\n",
      "583\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7985 - acc: 0.7500\n",
      "584\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9425 - acc: 0.7500\n",
      "585\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1301 - acc: 0.6250\n",
      "586\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6161 - acc: 0.9062\n",
      "587\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8112 - acc: 0.7812\n",
      "588\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9327 - acc: 0.6875\n",
      "589\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7022 - acc: 0.7812\n",
      "590\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8287 - acc: 0.7188\n",
      "591\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7936 - acc: 0.7188\n",
      "592\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3684 - acc: 0.7188\n",
      "593\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7091 - acc: 0.7812\n",
      "594\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7301 - acc: 0.7500\n",
      "595\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7757 - acc: 0.7500\n",
      "596\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8597 - acc: 0.7500\n",
      "597\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6711 - acc: 0.8438\n",
      "598\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8425 - acc: 0.8125\n",
      "599\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9591 - acc: 0.7500\n",
      "600\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9307 - acc: 0.7188\n",
      "601\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6911 - acc: 0.7188\n",
      "602\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1207 - acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "603\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8271 - acc: 0.7188\n",
      "604\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9991 - acc: 0.6875\n",
      "605\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8448 - acc: 0.7188\n",
      "606\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7840 - acc: 0.7812\n",
      "607\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8075 - acc: 0.6875\n",
      "608\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0471 - acc: 0.8750\n",
      "609\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6906 - acc: 0.8125\n",
      "610\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8192 - acc: 0.7500\n",
      "611\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3271 - acc: 0.5625\n",
      "612\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7122 - acc: 0.7500\n",
      "613\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7217 - acc: 0.8125\n",
      "614\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6074 - acc: 0.8438\n",
      "615\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.5060 - acc: 0.5938\n",
      "616\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0214 - acc: 0.7500\n",
      "617\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4785 - acc: 0.6250\n",
      "618\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7914 - acc: 0.8438\n",
      "619\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2032 - acc: 0.6875\n",
      "620\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0579 - acc: 0.7188\n",
      "621\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8389 - acc: 0.8125\n",
      "622\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1811 - acc: 0.7500\n",
      "623\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6973 - acc: 0.7500\n",
      "624\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5869 - acc: 0.8125\n",
      "625\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8191 - acc: 0.7188\n",
      "626\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8516 - acc: 0.8438\n",
      "627\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7917 - acc: 0.7812\n",
      "628\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7576 - acc: 0.8750\n",
      "629\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6456 - acc: 0.7500\n",
      "630\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.3518 - acc: 0.7812\n",
      "631\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7862 - acc: 0.7500\n",
      "632\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0063 - acc: 0.7500\n",
      "633\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5310 - acc: 0.9375\n",
      "634\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9549 - acc: 0.7188\n",
      "635\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5591 - acc: 0.8750\n",
      "636\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8476 - acc: 0.6875\n",
      "637\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.4044 - acc: 0.5938\n",
      "638\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0466 - acc: 0.7188\n",
      "639\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7882 - acc: 0.7812\n",
      "640\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8600 - acc: 0.7812\n",
      "641\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6582 - acc: 0.7812\n",
      "642\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7935 - acc: 0.7500\n",
      "643\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7659 - acc: 0.8438\n",
      "644\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0993 - acc: 0.6875\n",
      "645\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9677 - acc: 0.6875\n",
      "646\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7633 - acc: 0.6875\n",
      "647\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.3767 - acc: 0.9375\n",
      "648\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9774 - acc: 0.8438\n",
      "649\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7867 - acc: 0.7500\n",
      "650\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6703 - acc: 0.7500\n",
      "651\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7494 - acc: 0.7188\n",
      "652\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8745 - acc: 0.8438\n",
      "653\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.4914 - acc: 0.8438\n",
      "654\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7131 - acc: 0.7812\n",
      "655\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7094 - acc: 0.7812\n",
      "656\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5937 - acc: 0.7812\n",
      "657\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1940 - acc: 0.6562\n",
      "658\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8518 - acc: 0.7500\n",
      "659\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8515 - acc: 0.7812\n",
      "660\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7696 - acc: 0.7812\n",
      "661\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9290 - acc: 0.6250\n",
      "662\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8612 - acc: 0.8125\n",
      "663\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0663 - acc: 0.6875\n",
      "664\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.4433 - acc: 0.8750\n",
      "665\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8829 - acc: 0.7812\n",
      "666\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9652 - acc: 0.7188\n",
      "667\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0142 - acc: 0.7500\n",
      "668\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.5328 - acc: 0.8750\n",
      "669\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8289 - acc: 0.7188\n",
      "670\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2605 - acc: 0.5625\n",
      "671\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8283 - acc: 0.7812\n",
      "672\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6172 - acc: 0.8750\n",
      "673\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7868 - acc: 0.7188\n",
      "674\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9454 - acc: 0.7188\n",
      "675\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.1603 - acc: 0.6875\n",
      "676\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8244 - acc: 0.7812\n",
      "677\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8303 - acc: 0.7812\n",
      "678\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9684 - acc: 0.6875\n",
      "679\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0512 - acc: 0.6562\n",
      "680\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.7465 - acc: 0.8750\n",
      "681\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.0449 - acc: 0.7500\n",
      "682\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.3851 - acc: 0.8750\n",
      "683\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.1724 - acc: 0.6875\n",
      "684\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.8147 - acc: 0.8125\n",
      "685\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9433 - acc: 0.7500\n",
      "686\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.4166 - acc: 0.9375\n",
      "687\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3852 - acc: 0.8125\n",
      "688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7576 - acc: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9216 - acc: 0.7812\n",
      "690\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9610 - acc: 0.8438\n",
      "691\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.9072 - acc: 0.6875\n",
      "692\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.2423 - acc: 0.6250\n",
      "693\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6915 - acc: 0.7812\n",
      "694\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 1.3987 - acc: 0.6250\n",
      "695\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.0574 - acc: 0.6562\n",
      "696\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.9671 - acc: 0.7500\n",
      "697\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6893 - acc: 0.7500\n",
      "698\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.6811 - acc: 0.8750\n",
      "699\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8181 - acc: 0.7500\n",
      "700\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.6453 - acc: 0.8125\n",
      "701\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.5562 - acc: 0.8438\n",
      "702\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.7366 - acc: 0.8125\n",
      "703\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.8744 - acc: 0.7500\n",
      "704\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 56s 2s/step - loss: 1.2856 - acc: 0.7188\n",
      "705\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 26805240 into shape (32,231,1934,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5ceaf5adbfd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b22d4f226942>\u001b[0m in \u001b[0;36mmy_batch_generator\u001b[0;34m(batch_size, data_words, data_locs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhot_ind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    255\u001b[0m            [5, 6]])\n\u001b[1;32m    256\u001b[0m     \"\"\"\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 26805240 into shape (32,231,1934,1)"
     ]
    }
   ],
   "source": [
    "#opt = Adam(lr = 0.0001)\n",
    "#model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model = load_model('/Users/arushigupta/Desktop/iam_model.h5')\n",
    "losses = np.load('/Users/arushigupta/Desktop/losses.npy').tolist()#[ 3.8203 , 3.2832, 3.1751, 2.8531, 2.8936, 2.5651, 3.0994, 2.8175, 2.6513,3.0556] \n",
    "accs =   np.load('/Users/arushigupta/Desktop/accs.npy').tolist()#[ 0.0000e+00, 0.0625, 0.0625, 0.2812, 0.2500, 0.4688, 0.2188, 0.1875, 0.1875, 0.2500]\n",
    "for i in range(0, 100000000):\n",
    "    print(i)\n",
    "    x_t, y_t = next(data_gen)\n",
    "    history = model.fit(x_t, y_t, batch_size = 32)\n",
    "    losses.append(history.history['loss'][0])\n",
    "    accs.append(history.history['acc'][0])\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        model.save('/Users/arushigupta/Desktop/iam_model.h5')\n",
    "        np.save('/Users/arushigupta/Desktop/losses', losses)\n",
    "        np.save('/Users/arushigupta/Desktop/accs', accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.055600643157959]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [ 3.8203 , 3.2832, 3.1751, 2.8531, 2.8936, 2.5651, 3.0994, 2.8175, 2.6513,3.0556] \n",
    "\n",
    "acc = [ 0.0000e+00, 0.0625, 0.0625, 0.2812, 0.2500, 0.4688, 0.2188, 0.1875, 0.1875, 0.2500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.8203,\n",
       " 3.2832,\n",
       " 3.1751,\n",
       " 2.8531,\n",
       " 2.8936,\n",
       " 2.5651,\n",
       " 3.0994,\n",
       " 2.8175,\n",
       " 2.6513,\n",
       " 3.0556,\n",
       " [3.3048195838928223],\n",
       " [3.0254478454589844],\n",
       " [2.9447007179260254],\n",
       " [3.0857784748077393],\n",
       " [3.005197763442993],\n",
       " [2.80789852142334],\n",
       " [3.256582260131836],\n",
       " [3.3216419219970703],\n",
       " [2.88285493850708],\n",
       " [2.815941333770752]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossplot = np.load('/Users/arushigupta/Desktop/losses.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1455c8978>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146febb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( np.arange(0, len(lossplot)), lossplot)\n",
    "plt.title(\"Loss for Resnet 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accplot = np.load('/Users/arushigupta/Desktop/accs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x14b94d6a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14b58b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0, len(accplot)), accplot)\n",
    "plt.title(\"Accuracy for Resnet 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py:1269: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "model = load_model('/Users/arushigupta/Desktop/iam_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = np.load('/Users/arushigupta/Desktop/test_words.npy')\n",
    "test_ys =np.load('/Users/arushigupta/Desktop/test_ys.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['is'],\n",
       "       ['to'],\n",
       "       ['of'],\n",
       "       ...,\n",
       "       ['I'],\n",
       "       ['that'],\n",
       "       ['to']], dtype='<U5')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#so now for each test word we want to predict its value\n",
    "i = 0\n",
    "test_acc = 0\n",
    "for loc in test_ys:\n",
    "    first_im = loc[0]\n",
    "    img = misc.imread(first_im)\n",
    "    lab = test_words[i][0]\n",
    "    im_w = np.shape(img)[0]\n",
    "    im_h = np.shape(img)[1]\n",
    "    pad_0_l = ceil((max_width - im_w)/2.0)\n",
    "    pad_0_r = floor((max_width - im_w)/2.0)\n",
    "    pad_1_l  = ceil((max_height - im_h)/2.0)\n",
    "    pad_1_r = floor((max_height - im_h)/2.0)\n",
    "    padded_img =  np.pad(img, [(pad_0_l, pad_0_r),(pad_1_l, pad_1_r)], mode = 'constant', constant_values = 0)\n",
    "    probs = model.predict(np.reshape( padded_img, (1, 231, 1934, 1)) )\n",
    "    prediction = np.argmax(probs)\n",
    "    if prediction == hot_ind[lab]:\n",
    "        test_acc += 1\n",
    "    i += 1\n",
    "    \n",
    "print(1.0*test_acc/len(test_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231, 1934)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(padded_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict(np.reshape( padded_img, (1, 231, 1934, 1)) )\n",
    "prediction = np.argmax(probs)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_ind[lab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x155e94ac8>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12358b2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_im = test_ys[1][0]\n",
    "img = misc.imread(first_im)\n",
    "plt.imshow(img)\n",
    "im_w = np.shape(img)[0]\n",
    "im_h = np.shape(img)[1]\n",
    "pad_0_l = ceil((max_width - im_w)/2.0)\n",
    "pad_0_r = floor((max_width - im_w)/2.0)\n",
    "pad_1_l  = ceil((max_height - im_h)/2.0)\n",
    "pad_1_r = floor((max_height - im_h)/2.0)\n",
    "padded_img =  np.pad(img, [(pad_0_l, pad_0_r),(pad_1_l, pad_1_r)], mode = 'constant', constant_values = 0)\n",
    "plt.imshow(padded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_width,max_height,1))\n",
    "i = inp\n",
    "\n",
    "i = Conv2D(16, (3,3) , padding='same')(i)\n",
    "i = Activation('relu')(i)\n",
    "i = MaxPooling2D((2,2))(i)\n",
    "i = Conv2D(32, (3,3) , padding='same')(i)\n",
    "i = Activation('relu') (i)\n",
    "i = MaxPooling2D((2,2))(i)\n",
    "i = Conv2D(64, (3,3) , padding='same')(i)\n",
    "i = Activation('relu') (i)\n",
    "i = MaxPooling2D((2,2))(i)\n",
    "i = Flatten() (i)\n",
    "i = Dense(500)(i)\n",
    "i = Activation('relu')(i)\n",
    "i = Dense(26)(i)\n",
    "i = Activation('softmax')(i)\n",
    "modelII = Model(input=inp,output=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 231, 1934, 1)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 231, 1934, 16)     160       \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 231, 1934, 16)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 115, 967, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 115, 967, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 115, 967, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 57, 483, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 57, 483, 64)       18496     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 57, 483, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 28, 241, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 431872)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               215936500 \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 26)                13026     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 26)                0         \n",
      "=================================================================\n",
      "Total params: 215,972,822\n",
      "Trainable params: 215,972,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelII.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.0001)\n",
    "modelII.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 80s 2s/step - loss: 9.1046 - acc: 0.0000e+00\n",
      "1\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 21s 651ms/step - loss: 10.2179 - acc: 0.1875\n",
      "2\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 16s 513ms/step - loss: 11.1156 - acc: 0.2812\n",
      "3\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 380ms/step - loss: 12.9805 - acc: 0.1875\n",
      "4\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 330ms/step - loss: 11.0452 - acc: 0.3125\n",
      "5\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 329ms/step - loss: 13.0798 - acc: 0.1875\n",
      "6\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 328ms/step - loss: 11.0812 - acc: 0.3125\n",
      "7\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 16s 506ms/step - loss: 10.4857 - acc: 0.3438\n",
      "8\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 396ms/step - loss: 13.4330 - acc: 0.0938\n",
      "9\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 13.0960 - acc: 0.1875\n",
      "10\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 401ms/step - loss: 11.6357 - acc: 0.2500\n",
      "11\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 335ms/step - loss: 11.8465 - acc: 0.2188\n",
      "12\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 357ms/step - loss: 12.6369 - acc: 0.1875\n",
      "13\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 23s 712ms/step - loss: 11.8729 - acc: 0.2500\n",
      "14\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 352ms/step - loss: 11.5851 - acc: 0.2812\n",
      "15\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 298ms/step - loss: 11.6916 - acc: 0.2500\n",
      "16\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 305ms/step - loss: 13.0960 - acc: 0.1875\n",
      "17\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 23s 711ms/step - loss: 12.3458 - acc: 0.2188\n",
      "18\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 50s 2s/step - loss: 13.5997 - acc: 0.1562\n",
      "19\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 47s 1s/step - loss: 12.5923 - acc: 0.2188\n",
      "20\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 19s 599ms/step - loss: 11.4511 - acc: 0.2812\n",
      "21\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 366ms/step - loss: 12.0886 - acc: 0.2500\n",
      "22\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 362ms/step - loss: 12.8764 - acc: 0.1875\n",
      "23\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 317ms/step - loss: 12.5923 - acc: 0.2188\n",
      "24\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 11.9940 - acc: 0.2500\n",
      "25\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 12.1550 - acc: 0.2188\n",
      "26\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 408ms/step - loss: 15.0447 - acc: 0.0625\n",
      "27\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 12.5923 - acc: 0.2188\n",
      "28\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 14s 435ms/step - loss: 12.7738 - acc: 0.1875\n",
      "29\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 336ms/step - loss: 12.0886 - acc: 0.2500\n",
      "30\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 15s 458ms/step - loss: 11.0812 - acc: 0.3125\n",
      "31\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 330ms/step - loss: 11.0812 - acc: 0.3125\n",
      "32\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 28s 862ms/step - loss: 11.0812 - acc: 0.3125\n",
      "33\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 367ms/step - loss: 12.5931 - acc: 0.2188\n",
      "34\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 264ms/step - loss: 11.1830 - acc: 0.2812\n",
      "35\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 309ms/step - loss: 9.5701 - acc: 0.4062\n",
      "36\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 34s 1s/step - loss: 13.0960 - acc: 0.1875\n",
      "37\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 46s 1s/step - loss: 12.3570 - acc: 0.2188\n",
      "38\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 43s 1s/step - loss: 12.5923 - acc: 0.2188\n",
      "39\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 19s 579ms/step - loss: 12.5923 - acc: 0.2188\n",
      "40\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 350ms/step - loss: 13.0960 - acc: 0.1875\n",
      "41\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 360ms/step - loss: 11.8132 - acc: 0.2188\n",
      "42\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 318ms/step - loss: 11.0812 - acc: 0.3125\n",
      "43\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 325ms/step - loss: 12.0886 - acc: 0.2500\n",
      "44\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 339ms/step - loss: 10.5775 - acc: 0.3438\n",
      "45\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 368ms/step - loss: 13.0532 - acc: 0.1875\n",
      "46\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 339ms/step - loss: 12.0886 - acc: 0.2500\n",
      "47\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 395ms/step - loss: 11.5849 - acc: 0.2812\n",
      "48\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 319ms/step - loss: 10.6221 - acc: 0.3125\n",
      "49\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 16s 498ms/step - loss: 11.4117 - acc: 0.2812\n",
      "50\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 320ms/step - loss: 11.5849 - acc: 0.2812\n",
      "51\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 23s 721ms/step - loss: 10.0738 - acc: 0.3750\n",
      "52\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 16s 506ms/step - loss: 11.0517 - acc: 0.3125\n",
      "53\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 328ms/step - loss: 11.3726 - acc: 0.2812\n",
      "54\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 308ms/step - loss: 11.7535 - acc: 0.2500\n",
      "55\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 27s 834ms/step - loss: 13.0960 - acc: 0.1875\n",
      "56\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 51s 2s/step - loss: 13.0960 - acc: 0.1875\n",
      "57\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 44s 1s/step - loss: 11.5855 - acc: 0.2812\n",
      "58\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 25s 773ms/step - loss: 9.0674 - acc: 0.4375\n",
      "59\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 21s 657ms/step - loss: 10.0738 - acc: 0.3750\n",
      "60\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 15s 453ms/step - loss: 12.5923 - acc: 0.2188\n",
      "61\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 352ms/step - loss: 10.5780 - acc: 0.3438\n",
      "62\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 331ms/step - loss: 11.5849 - acc: 0.2812\n",
      "63\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 324ms/step - loss: 12.0886 - acc: 0.2500\n",
      "64\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 402ms/step - loss: 11.0821 - acc: 0.3125\n",
      "65\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 390ms/step - loss: 13.1675 - acc: 0.1562\n",
      "66\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 316ms/step - loss: 9.0664 - acc: 0.4375\n",
      "67\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 14s 445ms/step - loss: 10.5776 - acc: 0.3438\n",
      "68\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 310ms/step - loss: 12.0886 - acc: 0.2500\n",
      "69\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 23s 722ms/step - loss: 11.5849 - acc: 0.2812\n",
      "70\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 397ms/step - loss: 14.0288 - acc: 0.1250\n",
      "71\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 9s 280ms/step - loss: 10.8157 - acc: 0.3125\n",
      "72\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 297ms/step - loss: 12.0886 - acc: 0.2500\n",
      "73\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 30s 931ms/step - loss: 12.5923 - acc: 0.2188\n",
      "74\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 52s 2s/step - loss: 9.5652 - acc: 0.4062\n",
      "75\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 42s 1s/step - loss: 9.5701 - acc: 0.4062\n",
      "76\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 23s 711ms/step - loss: 10.5775 - acc: 0.3438\n",
      "77\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 17s 528ms/step - loss: 12.5923 - acc: 0.2188\n",
      "78\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 337ms/step - loss: 11.7918 - acc: 0.2500\n",
      "79\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 345ms/step - loss: 11.5855 - acc: 0.2812\n",
      "80\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 314ms/step - loss: 11.5849 - acc: 0.2812\n",
      "81\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 334ms/step - loss: 12.6533 - acc: 0.1875\n",
      "82\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 327ms/step - loss: 12.0886 - acc: 0.2500\n",
      "83\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 371ms/step - loss: 12.5923 - acc: 0.2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 343ms/step - loss: 12.0887 - acc: 0.2500\n",
      "85\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 10.5775 - acc: 0.3438\n",
      "86\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 321ms/step - loss: 10.5775 - acc: 0.3438\n",
      "87\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 305ms/step - loss: 13.3153 - acc: 0.1562\n",
      "88\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 16s 488ms/step - loss: 12.0899 - acc: 0.2500\n",
      "89\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 340ms/step - loss: 12.5923 - acc: 0.2188\n",
      "90\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 9s 275ms/step - loss: 12.5923 - acc: 0.2188\n",
      "91\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 9s 296ms/step - loss: 12.0887 - acc: 0.2500\n",
      "92\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 29s 913ms/step - loss: 11.5849 - acc: 0.2812\n",
      "93\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 53s 2s/step - loss: 10.7674 - acc: 0.3125\n",
      "94\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 49s 2s/step - loss: 10.9859 - acc: 0.3125\n",
      "95\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 24s 744ms/step - loss: 10.5775 - acc: 0.3438\n",
      "96\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 14s 448ms/step - loss: 11.1694 - acc: 0.2812\n",
      "97\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 369ms/step - loss: 9.5701 - acc: 0.4062\n",
      "98\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 319ms/step - loss: 13.0960 - acc: 0.1875\n",
      "99\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 315ms/step - loss: 13.5996 - acc: 0.1562\n",
      "100\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 321ms/step - loss: 11.5849 - acc: 0.2812\n",
      "101\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 412ms/step - loss: 13.0960 - acc: 0.1875\n",
      "102\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 330ms/step - loss: 11.5850 - acc: 0.2812\n",
      "103\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 328ms/step - loss: 12.4363 - acc: 0.2188\n",
      "104\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 297ms/step - loss: 12.4867 - acc: 0.2188\n",
      "105\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 334ms/step - loss: 10.0738 - acc: 0.3750\n",
      "106\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 17s 523ms/step - loss: 13.0960 - acc: 0.1875\n",
      "107\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 341ms/step - loss: 11.6375 - acc: 0.2500\n",
      "108\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 311ms/step - loss: 11.5849 - acc: 0.2812\n",
      "109\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 10s 308ms/step - loss: 12.0571 - acc: 0.2500\n",
      "110\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 31s 975ms/step - loss: 11.5849 - acc: 0.2812\n",
      "111\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 46s 1s/step - loss: 11.5849 - acc: 0.2812\n",
      "112\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 48s 2s/step - loss: 9.5701 - acc: 0.4062\n",
      "113\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 18s 572ms/step - loss: 13.0960 - acc: 0.1875\n",
      "114\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 350ms/step - loss: 10.5775 - acc: 0.3438\n",
      "115\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 331ms/step - loss: 11.5849 - acc: 0.2812\n",
      "116\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 349ms/step - loss: 11.5849 - acc: 0.2812\n",
      "117\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 332ms/step - loss: 11.5849 - acc: 0.2812\n",
      "118\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 14s 433ms/step - loss: 11.0812 - acc: 0.3125\n",
      "119\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 385ms/step - loss: 13.5996 - acc: 0.1562\n",
      "120\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 12s 387ms/step - loss: 13.4023 - acc: 0.1562\n",
      "121\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 14s 433ms/step - loss: 12.5923 - acc: 0.2188\n",
      "122\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 11s 357ms/step - loss: 13.5996 - acc: 0.1562\n",
      "123\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 410ms/step - loss: 12.5923 - acc: 0.2188\n",
      "124\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 13s 414ms/step - loss: 10.5775 - acc: 0.3438\n",
      "125\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 34s 1s/step - loss: 9.0665 - acc: 0.4375\n",
      "126\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 17s 526ms/step - loss: 11.0812 - acc: 0.3125\n",
      "127\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 9s 282ms/step - loss: 13.5996 - acc: 0.1562\n",
      "128\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 254ms/step - loss: 9.7291 - acc: 0.3750\n",
      "129\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 9.5701 - acc: 0.4062\n",
      "130\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 13.0960 - acc: 0.1875\n",
      "131\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 12.5923 - acc: 0.2188\n",
      "132\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 12.0886 - acc: 0.2500\n",
      "133\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 11.7107 - acc: 0.2188\n",
      "134\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 12.5923 - acc: 0.2188\n",
      "135\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 13.0960 - acc: 0.1875\n",
      "136\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 13.0960 - acc: 0.1875\n",
      "137\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 12.5923 - acc: 0.2188\n",
      "138\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 12.2570 - acc: 0.2188\n",
      "139\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 13.5996 - acc: 0.1562\n",
      "140\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 12.0886 - acc: 0.2500\n",
      "141\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 11.5849 - acc: 0.2812\n",
      "142\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 11.5670 - acc: 0.2812\n",
      "143\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 11.5849 - acc: 0.2812\n",
      "144\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 11.0780 - acc: 0.3125\n",
      "145\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 11.5849 - acc: 0.2812\n",
      "146\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 12.2470 - acc: 0.1875\n",
      "147\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 13.0830 - acc: 0.1250\n",
      "148\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 11.6245 - acc: 0.2500\n",
      "149\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 13.3712 - acc: 0.0938\n",
      "150\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 12.0920 - acc: 0.2500\n",
      "151\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 10.0738 - acc: 0.3750\n",
      "152\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 11.0818 - acc: 0.3125\n",
      "153\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 9.7634 - acc: 0.3750\n",
      "154\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 11.5850 - acc: 0.2812\n",
      "155\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 215ms/step - loss: 13.5996 - acc: 0.1562\n",
      "156\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 12.5923 - acc: 0.2188\n",
      "157\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 11.5849 - acc: 0.2812\n",
      "158\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 11.0812 - acc: 0.3125\n",
      "159\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 12.2910 - acc: 0.2188\n",
      "160\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 10.7155 - acc: 0.3125\n",
      "161\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 11.5849 - acc: 0.2812\n",
      "162\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 11.9005 - acc: 0.2188\n",
      "163\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 13.0960 - acc: 0.1875\n",
      "164\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 13.5996 - acc: 0.1562\n",
      "165\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 13.5996 - acc: 0.1562\n",
      "166\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 10.9019 - acc: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 14.6780 - acc: 0.0625\n",
      "168\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 12.0886 - acc: 0.2500\n",
      "169\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 10.8452 - acc: 0.3125\n",
      "170\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 10.9463 - acc: 0.3125\n",
      "171\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 9.5989 - acc: 0.3750\n",
      "172\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 11.3283 - acc: 0.2812\n",
      "173\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 10.3878 - acc: 0.3438\n",
      "174\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 10.6027 - acc: 0.3125\n",
      "175\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 13.5996 - acc: 0.1562\n",
      "176\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 9.9339 - acc: 0.3750\n",
      "177\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 10.0740 - acc: 0.3750\n",
      "178\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 11.2210 - acc: 0.2812\n",
      "179\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 7.9842 - acc: 0.5000\n",
      "180\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 10.5775 - acc: 0.3438\n",
      "181\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 10.5775 - acc: 0.3438\n",
      "182\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 10.3302 - acc: 0.3438\n",
      "183\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 11.0860 - acc: 0.3125\n",
      "184\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 10.8111 - acc: 0.3125\n",
      "185\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 11.0800 - acc: 0.3125\n",
      "186\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 10.6063 - acc: 0.3125\n",
      "187\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 12.0886 - acc: 0.2500\n",
      "188\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 11.0812 - acc: 0.3125\n",
      "189\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 8.5627 - acc: 0.4688\n",
      "190\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 11.9754 - acc: 0.2500\n",
      "191\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 11.0403 - acc: 0.3125\n",
      "192\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 10.5775 - acc: 0.3438\n",
      "193\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 10.9938 - acc: 0.2812\n",
      "194\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 11.0717 - acc: 0.2500\n",
      "195\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 10.2713 - acc: 0.3438\n",
      "196\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 10.7288 - acc: 0.3125\n",
      "197\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 10.1527 - acc: 0.3438\n",
      "198\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 12.8463 - acc: 0.1875\n",
      "199\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 12.9568 - acc: 0.1562\n",
      "200\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 11.3932 - acc: 0.2812\n",
      "201\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 11.8326 - acc: 0.2500\n",
      "202\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 13.0960 - acc: 0.1875\n",
      "203\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 7.8946 - acc: 0.5000\n",
      "204\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 10.1055 - acc: 0.3438\n",
      "205\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 12.4282 - acc: 0.2188\n",
      "206\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 9.8209 - acc: 0.3750\n",
      "207\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 14.1033 - acc: 0.1250\n",
      "208\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 9.0664 - acc: 0.4375\n",
      "209\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 11.0499 - acc: 0.2812\n",
      "210\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 9.9496 - acc: 0.3750\n",
      "211\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 10.0022 - acc: 0.3438\n",
      "212\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 11.8547 - acc: 0.2500\n",
      "213\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 12.0886 - acc: 0.2500\n",
      "214\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 11.8044 - acc: 0.2500\n",
      "215\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 10.3238 - acc: 0.3438\n",
      "216\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 11.8134 - acc: 0.2500\n",
      "217\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 13.0960 - acc: 0.1875\n",
      "218\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 12.9123 - acc: 0.1875\n",
      "219\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 12.7767 - acc: 0.1875\n",
      "220\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 10.5775 - acc: 0.3438\n",
      "221\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 10.7228 - acc: 0.3125\n",
      "222\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 10.4724 - acc: 0.2812\n",
      "223\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 11.0723 - acc: 0.3125\n",
      "224\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 10.1818 - acc: 0.3125\n",
      "225\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 8.8555 - acc: 0.3438\n",
      "226\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 10.9980 - acc: 0.2812\n",
      "227\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 10.5910 - acc: 0.3438\n",
      "228\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 10.5775 - acc: 0.3438\n",
      "229\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 12.3124 - acc: 0.2188\n",
      "230\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 9.3672 - acc: 0.4062\n",
      "231\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 10.4060 - acc: 0.2812\n",
      "232\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 7.5554 - acc: 0.5312\n",
      "233\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 12.5094 - acc: 0.2188\n",
      "234\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 9.7047 - acc: 0.3750\n",
      "235\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 10.4375 - acc: 0.3438\n",
      "236\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 10.5980 - acc: 0.3438\n",
      "237\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 8.5826 - acc: 0.4688\n",
      "238\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 10.0738 - acc: 0.3750\n",
      "239\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 11.2854 - acc: 0.2500\n",
      "240\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 9.2163 - acc: 0.4062\n",
      "241\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 247ms/step - loss: 12.0887 - acc: 0.2500\n",
      "242\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 9.6759 - acc: 0.3438\n",
      "243\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 10.0247 - acc: 0.3438\n",
      "244\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 12.3774 - acc: 0.1875\n",
      "245\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 12.5926 - acc: 0.2188\n",
      "246\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 9.5633 - acc: 0.3750\n",
      "247\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 8.6996 - acc: 0.4062\n",
      "248\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 11.5854 - acc: 0.2812\n",
      "249\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 10.8194 - acc: 0.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 244ms/step - loss: 10.0747 - acc: 0.3750\n",
      "251\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 10.6054 - acc: 0.3125\n",
      "252\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 245ms/step - loss: 13.8217 - acc: 0.1250\n",
      "253\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 244ms/step - loss: 9.1122 - acc: 0.4062\n",
      "254\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 243ms/step - loss: 6.7736 - acc: 0.5312\n",
      "255\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 10.5839 - acc: 0.3438\n",
      "256\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 9.5856 - acc: 0.3750\n",
      "257\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 12.0886 - acc: 0.2500\n",
      "258\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 10.5777 - acc: 0.3438\n",
      "259\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 11.0720 - acc: 0.2812\n",
      "260\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 8.9573 - acc: 0.4062\n",
      "261\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 12.2804 - acc: 0.2188\n",
      "262\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 10.2229 - acc: 0.3438\n",
      "263\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 10.2289 - acc: 0.3125\n",
      "264\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 10.6794 - acc: 0.2500\n",
      "265\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 12.8763 - acc: 0.1875\n",
      "266\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 9.8607 - acc: 0.3125\n",
      "267\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 9.5747 - acc: 0.4062\n",
      "268\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 8.6951 - acc: 0.4062\n",
      "269\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 11.0234 - acc: 0.2812\n",
      "270\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 10.9752 - acc: 0.2812\n",
      "271\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 8.7075 - acc: 0.4062\n",
      "272\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 11.0812 - acc: 0.3125\n",
      "273\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 9.6541 - acc: 0.3750\n",
      "274\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 8.5627 - acc: 0.4688\n",
      "275\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 9.0473 - acc: 0.4375\n",
      "276\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 10.7778 - acc: 0.3125\n",
      "277\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 9.1145 - acc: 0.4062\n",
      "278\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 11.2291 - acc: 0.2500\n",
      "279\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 10.3151 - acc: 0.3125\n",
      "280\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 11.8677 - acc: 0.2500\n",
      "281\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 9.2657 - acc: 0.4062\n",
      "282\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 11.5849 - acc: 0.2812\n",
      "283\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 9.5701 - acc: 0.4062\n",
      "284\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 12.5088 - acc: 0.2188\n",
      "285\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 11.1104 - acc: 0.2812\n",
      "286\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 10.3233 - acc: 0.3125\n",
      "287\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 9.3200 - acc: 0.3750\n",
      "288\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 10.0454 - acc: 0.2812\n",
      "289\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 10.1078 - acc: 0.3438\n",
      "290\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 11.1602 - acc: 0.2812\n",
      "291\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 9.2565 - acc: 0.4062\n",
      "292\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 8.2485 - acc: 0.4375\n",
      "293\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 10.3576 - acc: 0.3125\n",
      "294\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 12.7255 - acc: 0.1875\n",
      "295\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 10.7586 - acc: 0.2500\n",
      "296\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 10.5827 - acc: 0.3438\n",
      "297\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 9.5788 - acc: 0.3438\n",
      "298\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 243ms/step - loss: 10.3623 - acc: 0.3125\n",
      "299\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 8.5999 - acc: 0.4375\n",
      "300\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 10.2669 - acc: 0.3125\n",
      "301\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 247ms/step - loss: 10.0006 - acc: 0.3125\n",
      "302\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 8.5531 - acc: 0.3750\n",
      "303\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 12.6916 - acc: 0.1875\n",
      "304\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 6.5636 - acc: 0.5938\n",
      "305\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 6.4412 - acc: 0.4688\n",
      "306\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 9.9054 - acc: 0.3125\n",
      "307\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 8.0772 - acc: 0.4062\n",
      "308\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 9.3386 - acc: 0.3438\n",
      "309\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 9.1282 - acc: 0.3125\n",
      "310\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 10.2807 - acc: 0.2812\n",
      "311\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 8.9230 - acc: 0.4062\n",
      "312\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 8.0491 - acc: 0.3125\n",
      "313\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 8.8603 - acc: 0.3438\n",
      "314\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 9.2617 - acc: 0.3125\n",
      "315\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 7.2714 - acc: 0.4375\n",
      "316\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 9.0262 - acc: 0.3438\n",
      "317\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 9.5971 - acc: 0.3750\n",
      "318\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 9.5570 - acc: 0.2500\n",
      "319\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 8.9713 - acc: 0.2812\n",
      "320\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 243ms/step - loss: 6.4608 - acc: 0.5000\n",
      "321\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 10.7129 - acc: 0.1875\n",
      "322\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 9.0862 - acc: 0.1875\n",
      "323\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 7.0763 - acc: 0.3438\n",
      "324\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 6.4136 - acc: 0.3125\n",
      "325\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 7.1567 - acc: 0.3438\n",
      "326\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 6.7653 - acc: 0.3750\n",
      "327\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 7.1936 - acc: 0.2500\n",
      "328\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 6.0815 - acc: 0.2188\n",
      "329\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 247ms/step - loss: 5.9825 - acc: 0.2500\n",
      "330\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 6.5172 - acc: 0.2812\n",
      "331\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 7.8359 - acc: 0.2188\n",
      "332\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 5.9725 - acc: 0.2500\n",
      "333\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 5.6182 - acc: 0.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 245ms/step - loss: 4.1635 - acc: 0.1562\n",
      "335\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 4.8236 - acc: 0.2812\n",
      "336\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 246ms/step - loss: 2.8069 - acc: 0.3750\n",
      "337\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 243ms/step - loss: 3.3323 - acc: 0.3125\n",
      "338\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 246ms/step - loss: 3.7222 - acc: 0.2812\n",
      "339\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 2.7609 - acc: 0.4062\n",
      "340\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 2.9333 - acc: 0.3438\n",
      "341\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 251ms/step - loss: 3.4464 - acc: 0.2812\n",
      "342\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 3.2737 - acc: 0.2500\n",
      "343\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 2.6516 - acc: 0.3750\n",
      "344\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 2.4814 - acc: 0.4062\n",
      "345\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 2.3711 - acc: 0.4062\n",
      "346\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 2.4717 - acc: 0.2812\n",
      "347\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 2.3206 - acc: 0.5000\n",
      "348\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 3.1636 - acc: 0.3438\n",
      "349\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 2.8313 - acc: 0.2812\n",
      "350\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 243ms/step - loss: 2.6585 - acc: 0.3438\n",
      "351\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 2.5858 - acc: 0.3125\n",
      "352\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 2.2537 - acc: 0.2812\n",
      "353\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 2.8743 - acc: 0.3438\n",
      "354\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.5894 - acc: 0.2188\n",
      "355\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 2.5022 - acc: 0.3125\n",
      "356\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 2.4293 - acc: 0.4062\n",
      "357\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 2.2783 - acc: 0.3750\n",
      "358\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 2.4083 - acc: 0.4375\n",
      "359\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 247ms/step - loss: 2.4697 - acc: 0.2500\n",
      "360\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.8266 - acc: 0.5625\n",
      "361\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 246ms/step - loss: 2.6039 - acc: 0.3750\n",
      "362\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 2.4286 - acc: 0.3125\n",
      "363\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 2.4076 - acc: 0.3438\n",
      "364\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 246ms/step - loss: 2.3074 - acc: 0.4062\n",
      "365\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 2.2617 - acc: 0.3750\n",
      "366\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 2.3765 - acc: 0.3750\n",
      "367\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.1457 - acc: 0.4375\n",
      "368\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 2.2824 - acc: 0.4062\n",
      "369\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 2.0755 - acc: 0.4688\n",
      "370\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 2.1581 - acc: 0.3438\n",
      "371\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 2.3869 - acc: 0.3750\n",
      "372\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 2.2284 - acc: 0.3125\n",
      "373\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.8827 - acc: 0.5000\n",
      "374\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.6430 - acc: 0.2188\n",
      "375\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 2.4390 - acc: 0.3438\n",
      "376\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.1733 - acc: 0.4375\n",
      "377\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 2.4169 - acc: 0.2812\n",
      "378\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.9377 - acc: 0.3750\n",
      "379\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 244ms/step - loss: 2.2564 - acc: 0.3750\n",
      "380\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 1.9620 - acc: 0.4375\n",
      "381\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 245ms/step - loss: 2.0250 - acc: 0.3438\n",
      "382\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 232ms/step - loss: 2.3297 - acc: 0.3125\n",
      "383\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 2.0505 - acc: 0.4375\n",
      "384\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.7191 - acc: 0.4062\n",
      "385\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 2.4062 - acc: 0.2188\n",
      "386\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 2.1507 - acc: 0.3750\n",
      "387\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 2.3305 - acc: 0.3125\n",
      "388\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.5814 - acc: 0.6250\n",
      "389\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 2.5198 - acc: 0.2812\n",
      "390\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 2.4018 - acc: 0.2188\n",
      "391\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.9947 - acc: 0.3750\n",
      "392\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.3390 - acc: 0.4375\n",
      "393\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 2.1169 - acc: 0.3438\n",
      "394\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 2.1807 - acc: 0.4062\n",
      "395\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 2.0728 - acc: 0.4062\n",
      "396\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.0707 - acc: 0.4688\n",
      "397\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 2.2065 - acc: 0.2500\n",
      "398\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.3684 - acc: 0.4062\n",
      "399\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 1.7198 - acc: 0.5625\n",
      "400\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 2.0918 - acc: 0.4062\n",
      "401\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 234ms/step - loss: 2.2641 - acc: 0.2188\n",
      "402\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 2.5295 - acc: 0.2812\n",
      "403\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 2.2042 - acc: 0.4062\n",
      "404\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.6327 - acc: 0.4688\n",
      "405\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.8761 - acc: 0.3438\n",
      "406\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 1.5151 - acc: 0.5625\n",
      "407\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 2.2787 - acc: 0.3125\n",
      "408\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.6352 - acc: 0.5625\n",
      "409\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.7024 - acc: 0.5000\n",
      "410\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 232ms/step - loss: 1.9954 - acc: 0.5000\n",
      "411\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 2.0470 - acc: 0.4688\n",
      "412\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.9660 - acc: 0.4688\n",
      "413\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.0819 - acc: 0.4062\n",
      "414\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 2.1862 - acc: 0.4062\n",
      "415\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 1.9400 - acc: 0.4688\n",
      "416\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.9047 - acc: 0.4688\n",
      "417\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 2.2540 - acc: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 1.7732 - acc: 0.5625\n",
      "419\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 2.7959 - acc: 0.2812\n",
      "420\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 2.0074 - acc: 0.4375\n",
      "421\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 252ms/step - loss: 2.0358 - acc: 0.3750\n",
      "422\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 2.0481 - acc: 0.2812\n",
      "423\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.5989 - acc: 0.5625\n",
      "424\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.6928 - acc: 0.5625\n",
      "425\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.9557 - acc: 0.3125\n",
      "426\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 234ms/step - loss: 1.5539 - acc: 0.5938\n",
      "427\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 2.1061 - acc: 0.3125\n",
      "428\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 2.0784 - acc: 0.3750\n",
      "429\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.8781 - acc: 0.5000\n",
      "430\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.9894 - acc: 0.4062\n",
      "431\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 2.5324 - acc: 0.3750\n",
      "432\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 2.1218 - acc: 0.4688\n",
      "433\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 1.7364 - acc: 0.6250\n",
      "434\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 1.6765 - acc: 0.4062\n",
      "435\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 1.5588 - acc: 0.5312\n",
      "436\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 2.1037 - acc: 0.4375\n",
      "437\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 244ms/step - loss: 1.8715 - acc: 0.3438\n",
      "438\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.7877 - acc: 0.5312\n",
      "439\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 2.1030 - acc: 0.4375\n",
      "440\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 1.8409 - acc: 0.4688\n",
      "441\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 1.7223 - acc: 0.5000\n",
      "442\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 1.8612 - acc: 0.3750\n",
      "443\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 2.3110 - acc: 0.4375\n",
      "444\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 2.0741 - acc: 0.3438\n",
      "445\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.8301 - acc: 0.4375\n",
      "446\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 244ms/step - loss: 1.5730 - acc: 0.5000\n",
      "447\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.5929 - acc: 0.6250\n",
      "448\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 2.2396 - acc: 0.4688\n",
      "449\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 234ms/step - loss: 2.1062 - acc: 0.4062\n",
      "450\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 244ms/step - loss: 1.5050 - acc: 0.5312\n",
      "451\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 2.0164 - acc: 0.4062\n",
      "452\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 1.8548 - acc: 0.4062\n",
      "453\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 1.6936 - acc: 0.5000\n",
      "454\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 1.4487 - acc: 0.5625\n",
      "455\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 2.1770 - acc: 0.3438\n",
      "456\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 1.6797 - acc: 0.5000\n",
      "457\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 1.9218 - acc: 0.4688\n",
      "458\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 2.0088 - acc: 0.4375\n",
      "459\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 1.8255 - acc: 0.4688\n",
      "460\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.4458 - acc: 0.5312\n",
      "461\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 244ms/step - loss: 1.5126 - acc: 0.5938\n",
      "462\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 232ms/step - loss: 1.9343 - acc: 0.4062\n",
      "463\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 1.8668 - acc: 0.4688\n",
      "464\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 2.2346 - acc: 0.3438\n",
      "465\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 1.6018 - acc: 0.5625\n",
      "466\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 2.1998 - acc: 0.3750\n",
      "467\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.5299 - acc: 0.5312\n",
      "468\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.5607 - acc: 0.5312\n",
      "469\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 2.2759 - acc: 0.5312\n",
      "470\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.7606 - acc: 0.5938\n",
      "471\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 2.0697 - acc: 0.4375\n",
      "472\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.2831 - acc: 0.5938\n",
      "473\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 1.9576 - acc: 0.4062\n",
      "474\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.6702 - acc: 0.5625\n",
      "475\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.6324 - acc: 0.6250\n",
      "476\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.2988 - acc: 0.6562\n",
      "477\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.5124 - acc: 0.6562\n",
      "478\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.7743 - acc: 0.5938\n",
      "479\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.7053 - acc: 0.5000\n",
      "480\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 1.0622 - acc: 0.7500\n",
      "481\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 245ms/step - loss: 1.7447 - acc: 0.4375\n",
      "482\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 232ms/step - loss: 1.7885 - acc: 0.5000\n",
      "483\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 1.5691 - acc: 0.4375\n",
      "484\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 1.4759 - acc: 0.5625\n",
      "485\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.8186 - acc: 0.5625\n",
      "486\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 1.8972 - acc: 0.4375\n",
      "487\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 2.1053 - acc: 0.3750\n",
      "488\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.6082 - acc: 0.5625\n",
      "489\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.4174 - acc: 0.5312\n",
      "490\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.8362 - acc: 0.7500\n",
      "491\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.3675 - acc: 0.6250\n",
      "492\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.6642 - acc: 0.4688\n",
      "493\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 1.6180 - acc: 0.5000\n",
      "494\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 1.9717 - acc: 0.5000\n",
      "495\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.9480 - acc: 0.4062\n",
      "496\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.4092 - acc: 0.6562\n",
      "497\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.9445 - acc: 0.5938\n",
      "498\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.6189 - acc: 0.5000\n",
      "499\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 1.9837 - acc: 0.4062\n",
      "500\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 1.4654 - acc: 0.5938\n",
      "501\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 1.6981 - acc: 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 1.7607 - acc: 0.6250\n",
      "503\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 1.2788 - acc: 0.6250\n",
      "504\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.9022 - acc: 0.4062\n",
      "505\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.6853 - acc: 0.5938\n",
      "506\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 1.1991 - acc: 0.7188\n",
      "507\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 1.5609 - acc: 0.4062\n",
      "508\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.5183 - acc: 0.4062\n",
      "509\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 2.0726 - acc: 0.5312\n",
      "510\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 1.8381 - acc: 0.4688\n",
      "511\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.2438 - acc: 0.6250\n",
      "512\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.7857 - acc: 0.4375\n",
      "513\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.3856 - acc: 0.5938\n",
      "514\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 2.1648 - acc: 0.5312\n",
      "515\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.8282 - acc: 0.6250\n",
      "516\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 2.1377 - acc: 0.4375\n",
      "517\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 1.5187 - acc: 0.6250\n",
      "518\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 1.4499 - acc: 0.5938\n",
      "519\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.7327 - acc: 0.5312\n",
      "520\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 243ms/step - loss: 1.5192 - acc: 0.5625\n",
      "521\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 243ms/step - loss: 1.6408 - acc: 0.5312\n",
      "522\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 1.4686 - acc: 0.5312\n",
      "523\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 1.6301 - acc: 0.4062\n",
      "524\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.1169 - acc: 0.6562\n",
      "525\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 1.2783 - acc: 0.5625\n",
      "526\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 1.9128 - acc: 0.4375\n",
      "527\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.6466 - acc: 0.5000\n",
      "528\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.4409 - acc: 0.6250\n",
      "529\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 1.5790 - acc: 0.5000\n",
      "530\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 241ms/step - loss: 1.5452 - acc: 0.6250\n",
      "531\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.2905 - acc: 0.6250\n",
      "532\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.2874 - acc: 0.6250\n",
      "533\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 1.3737 - acc: 0.6250\n",
      "534\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 1.7235 - acc: 0.4688\n",
      "535\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 1.1102 - acc: 0.6875\n",
      "536\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.8708 - acc: 0.6562\n",
      "537\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.2537 - acc: 0.6562\n",
      "538\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.4661 - acc: 0.5938\n",
      "539\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.3333 - acc: 0.5938\n",
      "540\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.2278 - acc: 0.7500\n",
      "541\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.6682 - acc: 0.4375\n",
      "542\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 1.5642 - acc: 0.6250\n",
      "543\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.0856 - acc: 0.6875\n",
      "544\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.2972 - acc: 0.6875\n",
      "545\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.5348 - acc: 0.5000\n",
      "546\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.9049 - acc: 0.5000\n",
      "547\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.5798 - acc: 0.6250\n",
      "548\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.4751 - acc: 0.5625\n",
      "549\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.3117 - acc: 0.5312\n",
      "550\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 1.5864 - acc: 0.5625\n",
      "551\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.4413 - acc: 0.6250\n",
      "552\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.1196 - acc: 0.7188\n",
      "553\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.4634 - acc: 0.6562\n",
      "554\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.6942 - acc: 0.4688\n",
      "555\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.2655 - acc: 0.6562\n",
      "556\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 1.0978 - acc: 0.7500\n",
      "557\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.4475 - acc: 0.5000\n",
      "558\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.6099 - acc: 0.6562\n",
      "559\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 1.5188 - acc: 0.5000\n",
      "560\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.2314 - acc: 0.6562\n",
      "561\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 234ms/step - loss: 1.3777 - acc: 0.5938\n",
      "562\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 231ms/step - loss: 1.4549 - acc: 0.5938\n",
      "563\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.0568 - acc: 0.6562\n",
      "564\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.5049 - acc: 0.5312\n",
      "565\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.4070 - acc: 0.6562\n",
      "566\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.6331 - acc: 0.5312\n",
      "567\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.1899 - acc: 0.6562\n",
      "568\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.3234 - acc: 0.6875\n",
      "569\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.2077 - acc: 0.6562\n",
      "570\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.1412 - acc: 0.6562\n",
      "571\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.9009 - acc: 0.7188\n",
      "572\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.2689 - acc: 0.6875\n",
      "573\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.0932 - acc: 0.6562\n",
      "574\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.2240 - acc: 0.6250\n",
      "575\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.2074 - acc: 0.5938\n",
      "576\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.6743 - acc: 0.5312\n",
      "577\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.7288 - acc: 0.5312\n",
      "578\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.8447 - acc: 0.6875\n",
      "579\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.4881 - acc: 0.5000\n",
      "580\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.6339 - acc: 0.5625\n",
      "581\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 238ms/step - loss: 1.3372 - acc: 0.5625\n",
      "582\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 1.3688 - acc: 0.5938\n",
      "583\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.3609 - acc: 0.6250\n",
      "584\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.0691 - acc: 0.6875\n",
      "585\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.2728 - acc: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 1.2495 - acc: 0.5625\n",
      "587\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.6705 - acc: 0.5625\n",
      "588\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.0462 - acc: 0.6875\n",
      "589\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.1290 - acc: 0.6875\n",
      "590\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 2.0893 - acc: 0.3438\n",
      "591\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.2951 - acc: 0.6562\n",
      "592\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.6580 - acc: 0.5000\n",
      "593\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 216ms/step - loss: 1.1220 - acc: 0.6562\n",
      "594\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.8840 - acc: 0.7812\n",
      "595\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.3026 - acc: 0.5938\n",
      "596\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.7571 - acc: 0.8125\n",
      "597\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.7504 - acc: 0.3750\n",
      "598\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.2715 - acc: 0.7188\n",
      "599\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.4901 - acc: 0.5312\n",
      "600\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.8962 - acc: 0.4375\n",
      "601\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.2546 - acc: 0.6250\n",
      "602\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 1.1652 - acc: 0.5938\n",
      "603\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 1.4720 - acc: 0.5625\n",
      "604\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 0.9870 - acc: 0.6875\n",
      "605\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.2506 - acc: 0.5625\n",
      "606\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.4896 - acc: 0.6562\n",
      "607\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.2232 - acc: 0.6562\n",
      "608\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 1.1059 - acc: 0.7500\n",
      "609\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 1.2005 - acc: 0.6875\n",
      "610\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.5087 - acc: 0.5938\n",
      "611\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.0326 - acc: 0.7812\n",
      "612\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.2976 - acc: 0.5625\n",
      "613\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.1395 - acc: 0.7188\n",
      "614\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.7394 - acc: 0.6250\n",
      "615\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.0876 - acc: 0.7188\n",
      "616\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 1.0743 - acc: 0.6875\n",
      "617\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.5999 - acc: 0.5000\n",
      "618\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 1.5486 - acc: 0.5000\n",
      "619\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.2163 - acc: 0.6562\n",
      "620\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.2910 - acc: 0.7500\n",
      "621\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 1.1644 - acc: 0.7188\n",
      "622\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 1.2936 - acc: 0.6562\n",
      "623\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.1972 - acc: 0.7188\n",
      "624\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.1803 - acc: 0.5938\n",
      "625\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.1201 - acc: 0.6875\n",
      "626\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.2210 - acc: 0.6250\n",
      "627\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.0175 - acc: 0.8125\n",
      "628\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.5168 - acc: 0.5938\n",
      "629\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.4494 - acc: 0.5938\n",
      "630\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.0111 - acc: 0.7188\n",
      "631\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.9586 - acc: 0.7188\n",
      "632\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.1889 - acc: 0.5938\n",
      "633\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 0.9659 - acc: 0.7188\n",
      "634\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.2488 - acc: 0.6250\n",
      "635\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.2883 - acc: 0.5938\n",
      "636\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.4020 - acc: 0.6250\n",
      "637\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 1.2874 - acc: 0.6562\n",
      "638\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.3148 - acc: 0.6875\n",
      "639\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 1.5648 - acc: 0.5938\n",
      "640\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.7764 - acc: 0.7500\n",
      "641\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.6355 - acc: 0.5312\n",
      "642\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 230ms/step - loss: 0.8259 - acc: 0.7500\n",
      "643\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 1.0276 - acc: 0.7812\n",
      "644\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 1.0396 - acc: 0.7188\n",
      "645\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 226ms/step - loss: 1.0426 - acc: 0.6562\n",
      "646\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.3309 - acc: 0.5938\n",
      "647\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.8965 - acc: 0.7188\n",
      "648\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.0390 - acc: 0.6875\n",
      "649\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.3273 - acc: 0.5625\n",
      "650\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.6064 - acc: 0.5000\n",
      "651\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.8798 - acc: 0.7812\n",
      "652\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 1.2116 - acc: 0.5938\n",
      "653\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 218ms/step - loss: 1.0654 - acc: 0.6875\n",
      "654\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.0559 - acc: 0.6875\n",
      "655\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.5051 - acc: 0.6250\n",
      "656\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 0.8512 - acc: 0.8438\n",
      "657\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.0142 - acc: 0.7188\n",
      "658\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 216ms/step - loss: 0.9526 - acc: 0.7500\n",
      "659\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.2711 - acc: 0.6250\n",
      "660\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 0.8629 - acc: 0.7188\n",
      "661\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 236ms/step - loss: 1.1701 - acc: 0.6562\n",
      "662\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 234ms/step - loss: 0.8795 - acc: 0.7188\n",
      "663\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 0.8898 - acc: 0.7812\n",
      "664\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 228ms/step - loss: 1.1066 - acc: 0.7188\n",
      "665\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 0.9854 - acc: 0.6250\n",
      "666\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.0692 - acc: 0.7500\n",
      "667\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 1.3971 - acc: 0.5312\n",
      "668\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 227ms/step - loss: 1.0704 - acc: 0.7812\n",
      "669\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.4936 - acc: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 239ms/step - loss: 1.1540 - acc: 0.7188\n",
      "671\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.6910 - acc: 0.5312\n",
      "672\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 248ms/step - loss: 1.1697 - acc: 0.6875\n",
      "673\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 229ms/step - loss: 1.4491 - acc: 0.5625\n",
      "674\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 1.4062 - acc: 0.6875\n",
      "675\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 1.2677 - acc: 0.6562\n",
      "676\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 225ms/step - loss: 1.2034 - acc: 0.7188\n",
      "677\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.0611 - acc: 0.7500\n",
      "678\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 242ms/step - loss: 1.0324 - acc: 0.7500\n",
      "679\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 217ms/step - loss: 0.9213 - acc: 0.7812\n",
      "680\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 1.2769 - acc: 0.6562\n",
      "681\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.4207 - acc: 0.6875\n",
      "682\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 233ms/step - loss: 1.1289 - acc: 0.6562\n",
      "683\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 1.7062 - acc: 0.5625\n",
      "684\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.8188 - acc: 0.7812\n",
      "685\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 1.5730 - acc: 0.5938\n",
      "686\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 237ms/step - loss: 1.3770 - acc: 0.5938\n",
      "687\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 0.9917 - acc: 0.7188\n",
      "688\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 0.8133 - acc: 0.7188\n",
      "689\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 1.1488 - acc: 0.7500\n",
      "690\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 222ms/step - loss: 1.5043 - acc: 0.5625\n",
      "691\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 0.9925 - acc: 0.7188\n",
      "692\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.0889 - acc: 0.6875\n",
      "693\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 220ms/step - loss: 1.2447 - acc: 0.5625\n",
      "694\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 219ms/step - loss: 1.2792 - acc: 0.5938\n",
      "695\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 235ms/step - loss: 1.1310 - acc: 0.6250\n",
      "696\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.0678 - acc: 0.7188\n",
      "697\n",
      "Epoch 1/1\n",
      "32/32 [==============================] - 8s 240ms/step - loss: 1.1787 - acc: 0.7812\n",
      "698\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 26805240 into shape (32,231,1934,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-45f586e88fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelII\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlossesII\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-b22d4f226942>\u001b[0m in \u001b[0;36mmy_batch_generator\u001b[0;34m(batch_size, data_words, data_locs)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhot_ind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    255\u001b[0m            [5, 6]])\n\u001b[1;32m    256\u001b[0m     \"\"\"\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 26805240 into shape (32,231,1934,1)"
     ]
    }
   ],
   "source": [
    "#modelII = load_model('/Users/arushigupta/Desktop/iam_model.h5')\n",
    "lossesII = [ ]\n",
    "accsII =  [ ]\n",
    "for i in range(0, 100000000):\n",
    "    print(i)\n",
    "    x_t, y_t = next(data_gen)\n",
    "    history = modelII.fit(x_t, y_t, batch_size = 32)\n",
    "    lossesII.append(history.history['loss'][0])\n",
    "    accsII.append(history.history['acc'][0])\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        modelII.save('/Users/arushigupta/Desktop/iam_modelII.h5')\n",
    "        np.save('/Users/arushigupta/Desktop/lossesII', lossesII)\n",
    "        np.save('/Users/arushigupta/Desktop/accsII', accsII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22652"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116040.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26805240/231.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "116040/1934.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.875"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60/32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "test_accII = 0\n",
    "for loc in test_ys:\n",
    "    first_im = loc[0]\n",
    "    img = misc.imread(first_im)\n",
    "    lab = test_words[i][0]\n",
    "    im_w = np.shape(img)[0]\n",
    "    im_h = np.shape(img)[1]\n",
    "    pad_0_l = ceil((max_width - im_w)/2.0)\n",
    "    pad_0_r = floor((max_width - im_w)/2.0)\n",
    "    pad_1_l  = ceil((max_height - im_h)/2.0)\n",
    "    pad_1_r = floor((max_height - im_h)/2.0)\n",
    "    padded_img =  np.pad(img, [(pad_0_l, pad_0_r),(pad_1_l, pad_1_r)], mode = 'constant', constant_values = 0)\n",
    "    probs = modelII.predict(np.reshape( padded_img, (1, 231, 1934, 1)) )\n",
    "    prediction = np.argmax(probs)\n",
    "    if prediction == hot_ind[lab]:\n",
    "        test_accII += 1\n",
    "    i += 1\n",
    "    \n",
    "print(1.0*test_accII/len(test_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x16087fa90>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x160bfa908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lossplotII = np.load('/Users/arushigupta/Desktop/lossesII.npy')\n",
    "plt.plot( np.arange(0, len(lossplotII)), lossplotII)\n",
    "plt.title(\"Loss for basic CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1609898d0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x160855b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lossplotII = np.load('/Users/arushigupta/Desktop/accsII.npy')\n",
    "plt.plot( np.arange(0, len(lossplotII)), lossplotII)\n",
    "plt.title(\"Accuracy for basic CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
